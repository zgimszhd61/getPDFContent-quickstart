大型语言模型（LLMs）（例如欧阳等人，2022年；OpenAI等人，2023年）展现出在生成类似人类文本、回答问题和编程方面非凡能力。尽管它们取得了显著进展，但在要求高可靠性、安全性和道德一致性的任务中，这些模型面临挑战。为了解决这些挑战，人类反馈强化学习（RLHF），也称为基于偏好的强化学习（PbRL），提出了一个有前景的解决方案。这种通过人类反馈进行策略优化的框架，在Christiano等人的作品中得到了突出。

在2017年和最近的Ouyang等人(2022)的研究中，对指导遵循的大型语言模型进行精调取得了显著的实证成功，使它们更符合人类的偏好，从而更加实用。

目前大多数基于强化学习的人类反馈生成算法（RLHF）要么依赖明确的奖励模型，要么依赖隐式的奖励模型。以InstructGPT（欧阳等，2022）为例，首先建立一个参考策略πref，通常是通过监督预训练或基于指令的（监督）微调获得的。然后，通过利用Bradley-Terry（BT）模型（Bradley和Terry，1952）的人类偏好反馈数据训练奖励模型来获得显式的奖励函数。随后，采用诸如Proximal Policy Optimization（Schulman等，2017，PPO）这样的强化学习算法对参考语言生成模型πref进行微调，以最大化期望奖励函数。奖励模型为给定的回应y和提示x提供“奖励分数”r(y; x)，这大致反映了人类如何评价这些回应。最近出现了诸如Direct Preference Optimization（Rafailov等，2024，DPO）这样的方法。这些方法不再训练单独的奖励模型，而是使用对数似然比来隐式表示奖励分数，然后将其整合到相同的Bradley-Terry模型中，直接优化语言生成模型。尽管两步骤的RLHF算法和一步骤的直接偏好优化方法在实质上遵循奖励最大化目标，并由诸如BT模型之类的参数模型决定。

参数化偏好模型如布拉德利-特里模型（Bradley and Terry, 1952）和瑟斯通模型（Thurstone, 1927）提供了人类偏好的合理近似，但它们未能完全捕捉人类行为的复杂性。这些模型假设了不同选择之间的偏好关系是单调且传递的。然而，经验证据表明实际情况并非如此。例如，Tversky（1969）观察到人类决策会受到不同因素的影响，并表现出不一致性。这些观察表明，人类偏好并不总是遵循单一的、基于价值的等级体系，甚至可能表现出不合理之处，如在偏好关系中出现循环。对于线性最大化模型（LLMs），另一个推动力是Munos等人（2023年）实证证明，直接预测成对偏好可以比通过基于BT的奖励模型预测偏好实现更高的准确性。

为了解决人类偏好的不一致性，研究人员提出了直接处理偏好概率并设计可以更灵活表示人类偏好的算法，用于在排名或强化学习背景下（参考Lou等人，2022；Wu等人，2023）。最近，一系列新兴的研究（参考Munos等人，2023；Swamy等人，2024）还提出在这种一般偏好P(y≻y′|x)下研究LLMs的RLHF，其中y和y′分别是两个不同的响应，x是提示。Munos等人（2023）将RLHF表述为找到一个（Kullback-Leibler（KL）散度正则化的）两人恒量和游戏的纳什均衡，其中每个玩家都是输出响应并旨在最大化其被对手偏好的概率的LLM，其中偏好P(y≻y′|x)被假定为来自外部来源，如人类标注者或强大的语言模型。他们提出使用基于策略梯度的镜像下降算法来近似纳什均衡。最近，Swamy等人（2024）提出了自我对抗偏好优化（SPO）用于相同的（未正则化的）两人恒量和游戏。他们的算法旨在通过迭代地根据上一轮迭代中的策略生成的数据微调策略，以确定最小最大化的最优策略（即纳什均衡策略）。

他们的研究与我们的工作有所不同，他们专注于在简单的机器人或游戏任务中使用马尔可夫决策过程（MDP），并采用典型的策略优化算法，如PPO（Schulman等，2017）或SAC（Haarnoja等，2018）。目前尚不清楚他们的自我博弈框架如何应用于语言模型微调。最近，Rosset等人。

2024年提出了基于真实和预测胜率差异的交叉熵的直接纳什优化（DNO）算法。然而，他们的实际版本仍然采用了类似于徐等人（2023年）的迭代-DPO框架，而这一点并未在他们的理论分析中涉及到。

在这篇论文中，受到这些观察的启发，我们提出了一个新的自我对弈框架，该框架具有以下特点：（1）有确切的保证来解决双人零和博弈问题；（2）能够扩展到大规模高效地微调大型语言模型。具体来说，我们将RLHF问题形式化为一个零和双人博弈。我们的目标是找到纳什均衡策略，该策略在平均情况下始终提供首选响应。为了近似找到纳什均衡策略，我们采用了经典的在线自适应算法与乘法权重（Freund和Schapire，1999）作为解决双人博弈的高层框架。

此外，高层框架的每个步骤可以通过自我对弈机制来近似，即在每一轮中，策略通过在由策略生成并由偏好模型注释的合成数据上进行微调，在前一轮中与自身对战。

我们的贡献主要体现在以下几个方面： •我们从指数权重更新算法开始，这个算法被证明可以收敛到两人零和博弈的纳什均衡点，我们提出了自我对弈偏好优化（SPPO）算法用于大型语言模型对齐。该算法可以被证明会收敛到一个近似的纳什均衡点，并且采用了一种简单的损失函数形式，易于优化。

我们将我们的方法与最先进的方法进行比较，包括DPO、Identity Preference Optimization（IPO）(Azar等，2023)以及Kahneman-Tversky Optimization（KTO）（Etha- yarajh等，2024)，并且表明我们的SPPO损失函数能够有效地增加所选响应的对数似然，同时减少被拒绝响应的对数似然。这是对称配对损失（如DPO和IPO）所不能轻易实现的。我们的实验也证实，在各种基准测试中，SPPO优于迭代的DPO和IPO。

根据实证数据，SPPO显著提升了与Mistral-7B-Instruct-v0.2模型良好对齐的表现，在AlpacaEval 2.0（Dubois等人，2024a）测试集上，对抗GPT-4-Turbo的长度控制胜率增加了超过11%。此外，SPPO在不同任务中展现了强大的通用能力，包括MT-Bench、Open LLM排行榜和PairRM分数（Jiang等人，2023b）。与往往在优化至PairRM分数时在其他基准测试上表现下降的迭代式DPO/IPO不同，SPPO的表现提升是一致的。值得注意的是，所有强大的表现都是在没有来自GPT-4或其他更强大语言模型的外部监督（例如回复、偏好等）的情况下实现的。尽管仅使用来自UltraFeedback数据集（Cui等人，2023）的60k个提示（没有回复）且没有进行任何提示增强，我们的方法在AlpacaEval 2.0胜率上表现与GPT-4可比。

相关工作类似于RLHF，通过明确/隐含奖励模型进行。最初，由Christiano等人（2017年）提出了从人类反馈中进行强化学习（RLHF），该方法首先学习反映人类偏好的奖励模型，然后使用强化学习算法来最大化奖励。这一方法被Ouyang等人（2022年）应用于优化指示遵循的大型语言模型，并衍生出流行的ChatGPT。

以上作品中提到的奖励模型假设了一个参数模型，比如Bradley-Terry模型（Bradley和Terry，1952年），它分配了一个代表给定响应偏好程度的“分数”。最近，Rafailov等人（2024年）提议直接解决由Bradley-Terry模型隐含的分数的闭合形式解。直接策略优化（DPO）方法被称为更有效和稳定，然而，仍然隐含地假定了这样一个指定“分数”的奖励模型。在类似的精神中，赵等人（2023年）提出校准分数，使得比较中获胜者的分数比败者的分数高，并导致不同的SLic损失。同样，Ethayarajh等人（2024年）从Kahneman-Tversky人类效用函数推导出不同的损失函数（称为KTO），隐含地表示了给定响应的分数。刘等人（2023年）提出了拒绝采样优化（RSO）方法，利用偏好模型生成与候选人从最优策略采样的偏好对；然后在采样的偏好对上应用偏好优化。洪等人（2024年）提出了赔率比偏好优化（ORPO）算法，可以在一个训练会话中执行监督微调和偏好对齐，而无需维护中间参考策略。

在通用偏好模型中RLHF通常，人类的偏好并不严格遵循传递性，并且无法通过单个数字评分来充分表示。Azar等人（2023）提出了一种基于响应对之间的偏好概率而不是单个响应的分数的通用偏好优化目标。他们进一步提出了一种基于偏好概率的身份映射的学习目标，称为IPO（具有身份映射的偏好优化），其目的是最大化当前策略在给定参考策略上的预期获胜概率。Munos等人（2023）将通用偏好的RLHF问题制定为一个双人，恒定总和游戏，在这个游戏中，每个玩家都是一个旨在最大化其响应被对手偏好的概率的策略。他们旨在确定该游戏的纳什均衡策略，并提出了一种保证具有表格表示的策略的最后迭代收敛的镜像下降算法。Swamy等人（2024）研究了无KL正则化的双人恒定总和游戏，并提出了自我对弈偏好优化（SPO），这是一个类似于本文框架的RLHF框架。该框架与我们的框架在理想情况下使用指数权重更新的情况下是相同的。不同于我们的工作，他们关注机器人或游戏任务中的多轮马尔可夫决策过程（MDP），而不是对大型语言模型进行微调。最近，Rosset等人（2024）提出了基于真实和预测胜率差异之间的交叉熵的直接纳什优化（DNO）算法，并对有限样本逼近的误差提供了理论保证。然而，他们的实际版本仍然使用了类似于Xu等人（2023）的迭代-DPO框架，其中使用DPO损失而不是他们自己的DNO损失。值得注意的是，在他们的实验中，他们将GPT-4生成的响应添加为他们的由于表格表示，计算归一化因子是困难的，并且该算法通过抽样一个令牌而不是完整响应的方式进行近似执行。

他们将“金标准样本”融入到他们的微调数据中，并使用GPT-4作为裁判，为每个响应分配一个数字分数，用于构建偏好对。与此形成鲜明对比，我们的工作除了一个小型奖励模型外，不需要使用任何强大的外部监督。

自我对打微调
大多数上述研究（Rafailov等，2024年；赵等，2023年；阿扎尔等，2023年；Ethayarajh等，2024年）考虑从某个参考策略开始的单一优化过程。同样的过程可以以自我对打的方式重复应用多轮。在每一轮中，由上一轮获得的策略生成新数据；然后利用这些新数据训练一个新的策略，以超越旧策略。

自我对弈微调可以应用于两种情境，无论是有人类偏好数据还是没有。比如，Singh等人（2023）提出了一个期望最大化（EM）框架，在每一轮中生成新数据并标注奖励分数；通过在奖励高的数据上微调政策，得到新政策。Chen等人（2024）提出了一个自我对弈框架以监督方式微调模型。在每一轮中，通过将政策生成的回应标记为失败者，人类生成的回应标记为赢家，合成新的偏好对。然后在每一轮中应用DPO来根据这些合成的偏好数据微调另一个政策。Yuan等人（2024）提出了自奖励语言模型，语言模型本身被用来标注其自己回应的偏好。通过迭代DPO在这些标注数据上微调语言模型。这些研究表明迭代微调可以显著提高性能。

有一系列研究致力于分析逆强化学习反馈（RLHF）并提供其理论保证。朱等人（2023年）研究了标准的RLHF，包括单独的奖励学习和模型调整，并提出了一种悲观的奖励学习过程，可以证明学习出线性奖励模型。王等人（2024年）提出了一个框架，将具有奖励模型的任何RLHF问题简化为基于奖励的标准RL问题。此外，他们提出，在存在通用偏好模型时识别纳什均衡策略，并展示该问题可以简化为两方零和马尔可夫博弈。熊等人（2023年）研究了不同设置下的逆KL正则化背景下的上下文感知多臂老虎机问题，提出了具有有限样本理论保证的高效算法。叶等人（2024年）通过考虑离线和在线设置，研究了KL正则化纳什-从人类反馈学习（NLHF）的理论可学习性，并提出了可以证明高效的算法。季等人（2024年）提出了一种基于主动询问的近端策略优化算法，具有遗憾上界和查询复杂度，基于问题维度和次优差距。

3. 预备知识 我们将考虑以下的偏好学习场景。 给定一个文本序列（通常被称为提示）x= [x1, x2, . . .]，两个文本序列y= [y1, y2, . . .] 和y′作为对提示x生成的响应。 一个给定提示x的自回归语言模型π可以生成根据概率分解π(y|x) =NY i=1π(yi|x,y<i) 的响应y。

根据给定的提示 x 和两个反馈 y 和 y′，一个偏好预测者（可以是人类标注员或语言模型）会提供偏好反馈 o(y≻y′|x) ∈ {0,1}，表示 y 是否比 y′ 更受欢迎。我们将 P(y≻y′|x) = E[o(y≻y′|x)] 表示为 y 胜出 y′ 的概率。两个概率分布密度 p 和 q 的 KL 散度被定义为 KL(p∥q) = Ey∼p(y) [h log(p(y)/q(y)) ]。

在奖励模型 (RLHF) 中，Christiano等人（2017）首先学习一个遵循Bradley-Terry模型（Bradley和Terry，1952）的奖励函数 r(y;x)。对于一个提示-响应-响应三元组（x，y，y'），Bradley-Terry模型指定了 y 被选择而不是 y' 的概率为 P(y≻y′|x) = exp(r(y;x)) / (exp(r(y;x)) + exp(r(y′;x))) = σ(r(y;x)−r(y′;x))，其中σ(x) = ex/(ex+1) 是逻辑函数。与Bradley-Terry模型相关的奖励函数可以通过最大化对数似然 logP(y≻y′|x) 来估计。假设真实的奖励函数 r(y;x) 可以获得，Christiano等人（2017）建议通过使用RL中的策略优化算法（如PPO（Schulman等，2017））来解决以下优化问题：max θ Ex∼X,y∼πθ(·|x)[r(y;x)]−η−1Ex∼X[KL(πθ(·|x)∥πref(·|x))]，其中 X 是提示分布。

Rafailov等人(2024)指出上述优化问题具有封闭解，对于任何y，π∗(y|x)∝πref(y|x) exp( ηr(y;x))，这可以进一步转换为对于任何三元组 ( x,yw,yl) 的DPO损失，其中赢家yw胜过败者yl： ℓDPO(x,yw,yl;θ;πref) :=−logσ  η−1 logπθ(yw|x) πref(yw|x) −logπθ(yl|x) πref(yl|x)!。

根据王等人（2024年）和Munos等人（2023年）的研究，我们的目标是建立RLHF (Reinforcement Learning from Human Feedback) 方法，而不需要奖励/效用模型，因为人类偏好可能是非传递的（Tversky，1969年）。在一般性偏好预测器P(y≻y′|x)的指导下，我们遵循Dud´ ik等人（2015年）的研究，旨在确定von Neumann获胜者。更具体地说，von Neumann获胜者π*是以下两人恒定和游戏的（对称）Nash均衡点：（π*，π*）= arg max πmin π'∈Π Ex∼Xh Ey∼π(·|x), y'∼π'(·|x) [P(y≻y′|x)]。此外，我们定义了一种响应y在分布π的对抗下获胜的概率为P(y≻π|x) = Ey'∼π(·|x)[P(y≻y′|x)]，以及一种策略π击败另一种策略π'的概率P(π≻π'|x) = Ey∼π(·|x)Ey'∼π'(·|x)[P(y≻y′|x)]。

此外，我们定义 P(π≻π′) = Ex∼X[P(π≻π′|x)], 这里 x 是从提示分布 X 中抽取的提示。也许你会注意到由于偏好预测器的对称性，P(y′≻y|x) = 1 - P(y≻y′|x)，因此 P(π≻π′) + P(π′≻π) = 1。 两人零和博弈（3.3）可以简化为 (π∗, π∗) = arg max πmin π′P(π≻π′)。

在这一部分中，我们介绍了自我对弈偏好优化（SPPO）算法，该算法源自以下理论框架。

4.1 理论框架
已经有一些公认的算法可以在一个固定总和的两人博弈中近似求解纳什均衡。在这项工作中，我们遵循弗洛伊德和沙皮尔（1999年）的方法建立了一个迭代框架，可以渐进地收敛到平均最优策略。我们从一个理论框架开始，概念上解决两人博弈如下：πt+1(y|x)∝πt(y|x) exp( ηP(y≻πt|x)), for t= 1,2, . . . . (4.1)。公式（4.1）是一个迭代框架，依赖于每一轮t中的乘法权重更新，并且有一个清晰的结构。最初，我们有一个基础策略π1，通常来自于一些经过监督微调的模型。在每一轮中，更新的策略πt+1是从参考策略πt经过乘法权重更新得到的。更具体地说，如果一个响应y相对于当前策略πt有更高的平均优势，那么它应该具有更高的概率权重。

同样地，（4.1）可以写成 πt+1(y|x) =πt(y|x) exp ηP(y≻πt|x) Zπt(x)（4.2），其中 Zπt(x) =P yπt(y|x)exp ηP(y≻πt|x) 是归一化因子（也称为配分函数）。对于任何固定的 x 和 y，理想的更新策略 πt+1 应该满足以下等式：logπt+1(y|x) πt(y|x) =η·P(y≻πt|x)−logZπt(x)（4.3）。不同于 DPO 或 IPO 中取消 log 归一化因子 logZπt(x) 的一对一设计，通过对 y 和 y′ 之间的 (4.3) 进行微分，我们选择直接用 L2 距离来近似 (4.3)： πt+1= argmin πEx∼X,y∼πt(·|x) logπ(y|x) πt(y|x) − ηP(y≻πt|x)−logZπt(x)2 （4.4）。

7 概率的估计 优化目标（4.4）可以用有限样本来逼近。我们选择对每个提示 x 采样 K 个响应 y1，y2，...，yK ∼πt(·|x)，并将经验分布表示为 bπK t。有限样本优化问题可以近似为 πt+1= argmin πEx∼X,y∼πt(·|x) logπ(y|x) πt(y|x) − ηP(y≻bπK t|x)−logZbπK t(x)2 （4.5）。具体来说，P(y≻bπK t|x) = ∑K k=1P(y≻yk|x)/K 和 ZbπK t(x) =Ey∼πt(·|x)[exp(ηP(y≻bπK t|x))]。

随着使用新样本对ZbπK t(x)进行期望估计，并一共进行O(KB)次询问优先项来源P，有助于更准确地估计。(4.5) 这是一个高效可解的优化问题。简单来说，当K趋近于无穷大时，(4.5)将逐渐恢复到(4.4)的状态。我们对(4.4)的收敛性有以下保证：定理4.1。假设优化问题(4.4)是可实现的。通过(4.4)获得的策略为πtas，并通过混合策略 ¯πT=1 TPT t=1πt。通过设定 η= Θ(1 /√ T)，我们可以得到最大化π P(π≻¯πT) −最小化π P(π≺¯πT) =O(1/√ T)。

定理4.1表征了平均政策在时间跨度T内朝向纳什均衡收敛的速度，这是以对偶间隙为基础的。证明基于Freund和Schapire (1999)的定理1稍作修改。为了完整起见，我们在附录A中包含了证明过程。

作为替代方案，我们可以简单地用 η/22in(4.5) 替换掉估计的 logZbπK t(x)，以获得一个更清晰的目标： πt+1= argmin πEx∼X,y∼πt(·|x) logπ(y|x) πt(y|x) −η P(y≻bπK t|x)−1 22 。直觉上，如果出现平局（即P(y≻bπK t|x) = 1 /2），我们更希望模型不更新aty处的权重。如果y对bπK t 占据优势（即P(y≻bπK t|x)>1/2），那么我们会增加y处的概率密度，以利用y对bπK t的优势。在我们的实验中，我们选择最小化这个目标（4.6）。

4.2 基于上述理论框架，我们提出了基于自对弈偏好优化算法的 SPPO 算法。在算法1中，每一轮 t，算法1将根据 πt(·|x) 为每个提示 x 生成 K 个响应 y1，y2，. . . ，yK（第3行）。然后，在第4行，将查询偏好预测器 P 来计算 K 个响应之间的胜率。在第5行，可以应用一些标准来确定哪个响应应该保留在构建的数据集 Dt 中，并构建提示-响应-概率三元组 ( x,y,bP(y≻πt|x))。我们将在后面的第5节中讨论设计选择。一种直接的设计选择是将所有 K 个响应都包含在 Dt 中，每个 bP(yi≻πt|x) 都是通过将 y_i 与所有 K 个响应进行比较来估计的。总共将进行 O(K2) 次查询。

接着算法将会在数据集Dt（第6行）上优化（4.6）的结果。

假设任意一对之间的获胜概率都是一个公平的抛硬币，当 K 趋近无穷大时，我们可以证明 ZbπK t(x) 会趋近于 eη/2。

算法1 自我对弈偏好优化（SPPO）1：输入：基础策略πθ1，偏好预测器P，学习速率η，生成样本数量K。

第二步：对于t = 1,2, ...，执行以下步骤：
第三步：通过从分布X中采样x，以及从πt(·|x)中采样y1到K，生成合成响应。

4: 对于所有的k，k'∈[K]，注释胜率 P(yk≻yk′|x)。

从y1:K中选择回应来构建数据集Dt，其中i∈[N]，并且bP(yi≻πt|xi)。

6: 根据公式（4.6），优化πθt+1：θt+1←argmin θE(x,y,bP(y≻πt|x))∼Dt  log  πθ(y|x) πt(y|x)  −η  bP(y≻πt|x)−1 2  2  （4.7）。  
7: 结束 循环。与DPO、IPO和KTO相比，实际操作中，我们利用包含两个以上回应的小批量数据来估计给定回应的胜率，而DPO和IPO损失函数专注于单一回应对。当只有一对回应yw和yl可用时，我们可以基于偏好三元组（x，yw，yl）定义对称损失函数ℓSPPO(x,yw,yl;θ;πref)如下：logπθ(yw|x) πref(yw|x) −η  P(yw≻yl|x)−1 2  2  +logπθ(yl|x) πref(yl|x) −η  P(yw≺yl|x)−1 2  2  （4.8），其中P(yw≻yl|x)可以是在[0,1]内的软概率，也可以是指示yw≻yl的硬标签1。

现在我们来将SPPO损失与其他基准进行比较。为了便于比较，让a=βlog πθ(yw|x) πref(yw|x)，b=βlog πθ(yl|x) πref(yl|x)，c=βKL(πθ∥πref)，那么我们有ℓDPO(yw,yl,x) =−logσ(a−b)， ℓIPO(yw,yl,x) = [( a−b)−1]2， ℓKTO(yw,yl,x) =σ(−a+c) +σ(b−c)（简化），其中σ(x) =ex/(ex+ 1)，SPPO损失可以写为ℓSPPO(yw,yl,x) = (a−1/2)2+ (b+ 1/2)2。

可以看出，SPPO不仅努力缩小a和b之间的差距为1，还试图让a的值接近1/2，b的值接近-1/2，从而确保πθ(yw|x)> π ref(yw|x)和πθ(yl|x)< π ref(yl|x)。我们认为这点尤为重要：当有大量的偏好配对时，DPO和IPO可以确保政策将收敛到目标政策，但当偏好配对稀缺时（例如，每个提示只有一对配对），不能保证赢家a的估计奖励增加，输家b的估计奖励减少。

然而，只有赢家和输家之间的奖励差距（即a−b）会增加。Pal等人(2024)观察到这种现象，即DPO只会使输家的胜出可能性减小，而赢家的胜出可能性几乎不会改变。我们认为，直接将βlog πt+1(y|x) πt(y|x) 拟合到P(y≻πt|x)−1/2比试图将βlog πt+1(yw|x) πt(yw|x) −βlog πt+1(yl|x) πt(yl|x) 拟合到P(yw≻πt|x)−P(yl≻πt|x)更为直接。此外，SPPO与KTO具有类似的精神。KTO loss通过最小化σ(−a+c)使a变大，通过最小化σ(b−c)使b变小。相比之下，SPPO推动a变大至1/2，b变小至−1/2。

另一方面，我们想要指出，虽然DPO和KTO可以扩展到它们的迭代变体，但它们并非本质上是迭代算法，并且并没有明确保证它们可以达到纳什均衡。相比之下，SPPO和IPO是被设计为能够迭代地解决纳什均衡的。SPPO优于IPO，因为它的设计明确地缓解了数据稀疏问题，正如前面讨论的那样。

我们进行了5次实验，通过大量实验证明了我们的方法的性能，并将其与其他基准方法进行了比较。

5.1 实验设置基本模型和数据集 我们遵循 Snorkel3 的实验设置，该模型利用迭代式 DPO 来在 AlpacaEval 基准测试中实现最先进的性能。具体来说，我们使用 Mistral-7B-Instruct-v0.2 作为我们的基础模型4。 Mistral-7B-Instruct-v0.2 是 Mistral-7B-v0.2 模型（Jiang et al., 2023a）的指令微调版本。我们还采用 Ultrafeedback（Cui et al., 2023）作为我们的提示源，其中包括来自不同资源的约 60k 个提示。

在生成过程中，我们遵循Mistral-7B的标准聊天模板。为了在微调过程中避免过拟合，我们将数据集分成三部分，每次迭代只使用其中的一部分。

这些设置也被应用到模型Snorkel-Mistral-PairRM-DPO5（Snorkel）的训练中。

我们遵循Snorkel中的数据分割方法进行公平比较。

我们采用了PairRM模型(Jiang等人，2023b)，这是一个体积为0.4B的高效的两两偏好模型。PairRM基于DeBERTA-V3模型(He等人，2021)并在高质量的人类偏好数据集上训练。在像Auto-J Pairwise数据集(Li等人，2023a)这样的基准测试中，它表现出色，超越了大多数基于语言模型的奖励模型，并且和像UltraRM-13B(Cui等人，2023)这样更大的奖励模型有着类似的表现。我们建议读者在Huggingface的主页上查看详细的基准测试结果。因此，为了在准确性和效率之间取得平衡，我们将PairRM作为我们的排名模型，并遵循Snorkel的设计。

具体来说，PairRM将输出一个“相对奖励”s(y,y′;x)，反映了y和y′之间的强度差异，即 P(y≻y′|x) =exp(s(y,y′;x)) 1 + exp(s(y,y′;x))。

这里提供了一些链接到 Hugging Face 预训练模型的地址：

1. Snorkel-Mistral-PairRM-DPO 模型：https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO
2. Mistral-7B-Instruct-v0.2 模型：https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
3. Snorkel-Mistral-PairRM-DPO 模型：https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO
4. PairRM 模型：https://huggingface.co/llm-blender/PairRM

与基于 Bradley-Terry 的奖励模型不同，PairRM 模型只分配相对奖励，不保证是传递的（即，s(y1,y2;x) + s(y2,y3;x) ≠ s(y1,y3;x)）。因此，它确实模拟了一般的偏好。

在生成阶段的每次迭代中，我们使用top p=1.0和温度1.0从当前策略中抽样。我们使用不同的随机种子进行抽样，以获得每个提示的K=5个不同的回应。之前使用迭代DPO的研究选择2个回应来为每个提示形成一对。为了公平比较，我们在偏好数据中不包括所有的K=5个回应，而是从中选择两个回应。按照Snorkel的做法，我们选择得分最高和最低的PairRM分别作为胜者yw和败者yl的回应，对于每个回应yi，其PairRM得分定义如下：sPairRM(yi;x) := 1/K Σ(k=1->K)s(yi,yk;x)。

我们通过对所有抽样响应的平均胜率来估算分布中的胜率，具体方法如下(4.5)所述：对于所有的i∈[K]，计算公式为bP(yi≻πt|xi) = 1/K Σk=1K P(yi≻yk|x)。

超参数调整实验是在8块Nvidia A100 GPU上进行的。对于SPPO算法，我们总共训练了三次迭代。在每次迭代中，我们选择了在UltraFeedback的2万条提示的第一个时期上训练的模型，继续进行下一次迭代。全局训练批量大小设置为64，η设置为1 e3。学习率调度由以下超参数确定：学习率=5.0e-7，总训练时期数=18，预热比例=0.1，线性调度。每个模型的最佳超参数是通过在Ultrafeedback的保留子集上使用PairRM-0.4B作为度量标准的平均胜率来选择的。有关使用PairRM作为评判标准进行胜率比较的更多细节，请参考第5.2节和图3。

基准线 我们评估以下基本模型以及Fei-调L语言模型的基准方法: Mistral-7B-Instruct-v0.2: Mistral-7B-Instruct-v0.2是Mistral-7B-v0.2模型（Jiang等人，2023年）的指令微调版本。它是我们算法的起点。

•潜水设备（Mistral-PairRM-DPO）：我们直接在Hugging-Face7上评估了上传的检查点。 这个模型是通过从Mistral-7B-Instruct-v0.2开始进行三轮迭代的DPO获得的。

我们还自己实现了迭代式 DPO 算法。实验设置和模型选择方案与我们用于 SPPO 的相同，唯一不同的是采用了在（4.9）中定义的 DPO 损失函数。超参数经过优化，以最大化每次迭代由 PairRM 评估的平均胜率。需要注意的是，Rosset 等人（2024年）中的实际算法本质上与迭代式 DPO 相同。

在这个链接中，我们介绍了一种名为Snorkel-Mistral-PairRM-DPO的模型。我们自己实现了迭代的投票/标签模糊化（IPO）算法。实验设置和模型选择方案与迭代的DPO相同，只是损失函数为IPO损失（4.10）。为了公平比较，IPO的超参数也是通过在Ultrafeedback的保留子集上评估使用平均PairRM胜率来选择的。

•自我奖励的LLM：Yuan等人（2024年）提出通过使用LLM本身作为偏好评判器来构建新的偏好对，并采用DPO算法来进行迭代微调LLM。我们使用Yuan等人（2024年）报告的AlpacaEval 2.0胜率来进行比较。

请注意自奖励的语言模型是从Llama 2 70B训练而来的。

基准测试
和之前的研究一样，我们使用AlpacaEval 2.0（Dubois等人，2024a）、MT-Bench（Zheng等人，2024）和Open LLM排行榜（Beeching等人，2023a）作为我们的评估基准。

•AlpacaEval 2.0 是基于LLM的自动评估基准。它采用了AlpacaFarm（Dubois等，2024b）作为其提示集，其中包含一般的人类指示。模型生成的回答和由GPT-4-Turbo生成的参考答案将被输入到基于GPT-4-Turbo的注释器中进行评判。我们遵循标准方法，并报告相对于参考答案的胜率。

MT-Bench（郑等人，2024年）包含80个高质量的多轮开放式问题。 这些问题涵盖写作、角色扮演、数学、编程等主题。生成的答案由GPT-4评判，并直接给出一个分数，而无需两两比较。

•Open LLM Leaderboard（Beeching等人，2023a）包括六个数据集，每个数据集都专注于语言模型评估的一个方面。具体而言，评估标准包括数学问题解决、语言理解、模仿人类错误以及推理。我们遵循标准的评估流程，使用上下文学习来提示语言模型，并计算六个数据集的平均分数以衡量性能。

5.2 实验结果
我们在上述三个基准数据集上评估了模型。我们还比较了基于预训练偏好模型PairRM的模型。

在评估 AI 聊天机器人时，人工评价仍然是衡量质量和准确性的基准。然而，由于人工评价在可扩展性和可再现性方面存在局限，我们探索了使用 GPT-4 的先进能力作为自动评估工具的替代方法。我们使用 GPT-4 进行基于 AlpacaEval 2.0 和 MT-Bench 的自动评估，以衡量我们模型的聊天机器人能力。AlpacaEval 2.0 的结果可见于表1，MT-Bench 的结果在图2（左侧）中展示。我们还提供了一个雷达图，分析了 MT-Bench 的结果，可见于图2（右侧）。我们发现，随着迭代对齐次数的增加，SPPO 模型的性能持续提高。

表格1（AlpacaEval 2.0）展示了在805个提示上不同模型相对于GPT-4-Turbo基准线的获胜率。我们还包括了一列表示长度控制下的获胜率，表1中AlpacaEval 2.0对各种模型（在基准线中有详细描述）的评估，以百分比（%）表示常规和长度控制（LC）获胜率。 SPPO Iter3模型实现了最高的LC获胜率为28.53%，常规获胜率为31.02%。 SPPO模型在迭代过程中表现稳定，并且胜过其他基准线，其他基准线倾向于生成较长的回复。此外，在测试时使用PairRM奖励模型（16个中挑选最好）进行重新排序持续增强了所有模型以及SPPO的性能，而SPPO（16个中挑选最好）在无需像GPT-4那样强有力的外部监督的情况下取得了高获胜率。

ModelAlpacaEval 2.0是一个评估模型表格，展示了不同模型的胜率以及平均获胜长度。其中包括了Mistral-7B-Instruct-v0.2、Snorkel、Self-Rewarding、DPO、IPO以及SPPO等模型在不同迭代阶段的表现数据。这些数据显示了每个模型的表现情况，还对模型的平均输出长度进行了比较，考虑了以LLM为基础的评判者倾向于偏好更长序列输出的趋势，这个问题在口语中被称为“奖励破解”现象。

根据表格显示，SPPO的第三次迭代在长度控制版本中拥有最高的胜率，为28.52%，整体胜率为31.02%。相比之前的迭代，性能提升分别为7.69% (Mistral-7B-Instruct →Iter1)、2.10% (Iter1 →Iter2)和1.64% (Iter2 →Iter3)，显示出迭代过程中稳步改进的趋势，如图1所示。此外，数据表明相较于DPO和IPO的迭代变体，SPPO表现更加优异。SPPO的长度控制胜率达到了28.53%，优于DPO的最佳26.39%（由Snorkel提供）和IPO的25.45%。值得注意的是，尽管DPO和IPO训练往往会显著增加平均输出长度（分别为2736和2654），而SPPO显示出更为适度的长度增加，在基础模型的1676的基础上第三次迭代时增加到2163。

这说明SPPO提高了性能，同时更有效地控制了趋势。AlpacaEval 2.0排行榜结果显示了正常和长度受控(LC)的胜率百分比(%）。SPPO可以胜过更大的模型，而SPPO(16个中最好的)可以胜过专有模型，比如GPT-4(6/13)。

ModelAlpacaEval 2.0 LC. 胜率胜率 GPT-4 Turbo 50.0 50.0 Claude 3 Opus 40.5 29.1 GPT-4 0314 35.3 22.1 Llama 3 70B Instruct 34.4 33.2 SPPO Iter3 (16 强赛制最佳) 32.1 34.9 GPT-4 0613 30.2 15.8 Snorkel (Mistral-PairRM-DPO 16 强赛制最佳) 30.0 34.9 Mistral Medium 28.6 21.9 SPPO Iter3 28.5 31.0 Claude 2 28.2 17.2 Snorkel (Mistral-PairRM-DPO) 26.4 30.2 Gemini Pro 24.4 18.2 Mistral 8 ×7B v0.1 23.7 18.1 Llama 3 8B Instruct 22.9 22.6 GPT-3.5 Turbo 0613 22.7 14.1 Vicuna 33B v1.3 17.6 12.7 倾向于更长的输出长度，相较于 DPO 和 IPO。最后，我们展示了每个模型的16强赛制最佳结果，通过使用 PairRM 奖励模型进行选择。我们发现，在测试时使用偏好模型重新排名可以持续提高基础模型（Mistral-7B-Instruct-v0.2）、DPO（Snorkel）和SPPO（Iter3）的表现，分别提高了5.34％、3.57％和3.6％。值得注意的是，这表明，虽然 SPPO 利用 PairRM-0.4B 作为唯一外部监督明显增强了模型对齐性，但并没有导致过度优化对抗偏好模型（Gao等，2023）。未来的工作将探索模型对齐的进一步改进，可能通过超越目前三轮（遵循 Snorkel 的方法）的额外迭代。

在表2中，我们将AlpacaEval 2.0排行榜上的SPPO与其他最先进的人工智能聊天机器人进行了比较。我们发现我们的SPPO模型在测试私有对齐数据集训练的许多竞争模型（例如，Claude 2、Gemini Pro和Llama 3 8B Instruct）中表现更好。通过测试时间的重新排序，SPPO Iter3（16个模型中的最佳模型）甚至可以与GPT-4 0613和Llama 3 70B Instruct竞争。

在图2（左侧）中，我们评估了SPPO在MT-Bench上的表现。可以看到，SPPO Iter3的表现优于所有基准模型，取得了平均得分7.59。虽然我们不确定为什么在前两次迭代中MT-Bench的表现会下降，但SPPO在最后一次迭代的表现仍然优于基准模型。由于长度受控的AlpacaEval 2.0与人类评价有98%的皮尔逊相关性，并且有着10倍的评估提示，它很可能提供比MT-Bench更可靠的评估。为了更深入地了解MT-Bench的表现，我们在图2（右侧）中对改进进行了分类，按问题提示类别进行了划分。

图1显示了在AlpacaEval 2.0上，使用（a）和不使用（b）长度控制（LC）时，SPPO与GPT-4-Turbo之间的胜率。SPPO在LC和原始胜率上都展示了稳定的改善。

表3: 开放式LLM排行榜评估。 使用SPPO微调后，提高了基础模型在Arc, TruthfulQA和GSM8k上的性能，达到了最先进的平均分数66.75。

然而，我们研究中模拟的PairRM偏好模型显示，随后的DPO、IPO和SPPO迭代表现出下降趋势。可能会发生与人类偏好对齐并不总是会提升整体性能，甚至会削弱性能的情况。

以下是不同模型在各项任务中的平均表现得分：Models Arc TruthfulQA WinoGrande GSM8k HellaSwag MMLU Average Mistral-7B-Instruct-v0.2 63.65 66.85 77.98 41.93 84.89 59.15 65.74 Snorkel 66.04 70.86 77.74 36.77 85.64 60.83 66.31 DPO Iter1 63.14 68.39 77.19 40.33 85.25 59.41 65.62 DPO Iter2 64.16 67.84 76.09 39.95 85.23 59.03 65.38 DPO Iter3 65.19 67.89 77.27 32.30 85.49 59.00 64.52 IPO Iter1 64.68 68.60 77.98 43.75 85.08 59.04 66.52 IPO Iter2 62.12 66.30 77.51 39.20 83.15 59.70 64.66 IPO Iter3 62.97 67.12 77.51 37.45 83.69 59.57 64.72 SPPO Iter1 65.02 69.40 77.82 43.82 85.11 58.84 66.67 SPPO Iter2 65.53 69.55 77.03 44.35 85.29 58.72 66.75 SPPO Iter3 65.36 69.97 76.80 42.68 85.16 58.45 66.40 SPPO Iter3在角色扮演、推理、数学和编码任务中表现出显著的提升。

我们使用Huggingface开放的LLM排行榜（Beeching等，2023b）进一步评估SPPO模型的能力。该排行榜包括6个不同的数据集，每个数据集都专注于LLMs的特定能力：Arc（Clark等，2018），HellaSwag（Zellers等，2019），Winogrande（Sakaguchi等，2021），MMLU（Hendrycks等，2020），TruthfulQA（Lin等，2021）和GPS8k（Cobbe等，2021）。这些模型会根据零样本或少样本示例进行提示。

表3中的结果表明，SPPO可以提高基础模型在Arc，TruthfulQA和GSM8k上的性能，并且在15ModelMT-Bench 1st Turn 2nd Turn Average Mistral-7B-Instruct-v0.2 7.78 7.25 7.51 Snorkel（Mistral-PairRM-DPO) 7.83 7.33 7.58 DPO Iter1 7.45 6.58 7.02 DPO Iter2 7.57 6.56 7.06 DPO Iter3 7.49 6.69 7.09 SPPO Iter1 7.63 6.79 7.21 SPPO Iter2 7.90 7.08 7.49 SPPO Iter3 7.84 7.34 7.59。在MT-Bench评估中，左图显示SPPO Iter3通过实现7.59的平均分数，优于所有基准模型。尽管在前两次迭代中性能出现了初步下降，但SPPO Iter3在最后一次迭代中改进了基础模型。右图展示了MT-Bench结果的雷达图，对比各个模型的表现。

SPPO第三轮在不同MT-Bench类别中有所改善，表现出在角色扮演、推理、数学和编码任务中取得了显著的进展。

在对齐的第一个迭代中，平均得分为66.75。然而，在随后的对齐迭代中，DPO、IPO和SPPO的表现在第一或第二次迭代后下降。这一限制可能归因于“对齐税”现象（Askell等人，2021），这表明与人类偏好对齐（在我们的研究中由PairRM偏好模拟）可能不会提高甚至损害总体表现。通过对齐迭代改进语言模型能力仍然是未来研究的一个课题，我们认为纳入高质量的SFT注释（Chen等人，2024）可能在这一努力中发挥重要作用。

在SPPO被确定为诺依曼冠军（见（3.3））的两人零和博弈中，我们使用PairRM作为评判标准，来检测SPPO模型与其他基线模型之间的成对偏好。PairRM测量的成对胜率如图3所示。我们观察到，在所有算法（也就是DPO、IPO和SPPO）中，新模型的迭代版本都胜过之前的版本。例如，SPPO第3轮的表现优于SPPO第2轮。SPPO和IPO在所有迭代中一直优于DPO。尽管在前两轮中，SPPO优于IPO，在最后一轮中，IPO的表现又超过了SPPO。考虑到SPPO在由GPT-4评估的标准基准测试中表现优异，以及在与真实答案（例如AlpacaEval 2.0、MT-Bench和Open LLM排行榜）对比中的表现，结合IPO倾向于产生更长序列输出（见表1中的平均长度），我们认为这是由于IPO利用PairRM中倾向于更长序列的长度偏好所致。相反，SPPO模型受益于更稳健的正则化，同时采用了乘法权重更新框架。

5.3 消融研究

在估计胜率P(y≻πt|x)时，我们研究了小批量大小的影响。具体来说，对于每个提示，我们仍然生成5个响应，并根据数据中的具体值选出获胜者yw和输家yl。图3展示了基础模型（Mistral-7B-Instruct-v0.2）、DPO模型、IPO模型和SPPO模型之间的两两胜率，其中使用PairRM-0.4B作为评判标准，这可能有利于具有更长输出的模型。在具有更强大评判模型（例如GPT-4）的基准测试中，如AlpacaEval 2.0和MT-Bench，SPPO表现远高于其他基线算法。

PairRM得分。在估计概率时，我们将批量大小变化为K= 2,3,5。对于K= 2，我们仅使用2个样本yw和yl来估计P(y≻πt|x)，即bP(yw≻πt|x) =P(yw≻yw|x) +P(yw≻yl|x) 2=1/2 +P(yw≻yl|x) 2，以及bP(yl≻πt|x)以类似的方式计算。而K= 5 表示我们使用的原始设置。

我们比较了AlpacaEval 2.0的结果，如图4所示。我们发现，SPPO的性能对于估算P(y≻πt|x)时的噪音是稳健的。尽管在最初的迭代中K=5的性能胜过了K=2，但在随后的迭代中它们之间的性能差距逐渐减小。

另外，我们观察到当K=2时，输出长度增长的趋势有所减弱。

结论：本文介绍了自我对弈优化（SPPO），这是一种创新方法，用于通过人类/AI反馈微调大型语言模型（LLMs）。我们的方法进行自我对弈17迭代，每个迭代都有不同的迷你批大小。迭代1、迭代2和迭代3的结果显示了AlpacaEval 2.0的胜率和平均长度（字符）。我们比较了不同迷你批大小的SPPO对AlpacaEval 2.0的评估，包括正常和长度控制（LC）的胜率百分比。K=2和K=5表示在估计胜率P(y≻πt|x)时使用的不同迷你批大小。

在一个双人游戏中，通过基于偏好的学习目标，逐步改进模型以接近纳什均衡点。SPPO在诸如AlpacaEval 2.0、MT-Bench和Open LLM Leaderboard等多个基准测试中展现出了明显的改进，超过了现有方法如DPO和IPO。通过整合偏好模型并采用批处理估算过程，SPPO使LLMs更贴近人类偏好，并避免了“长度偏差”奖励欺骗等常见问题。这些结果强调了SPPO提升生成式人工智能系统对齐性的潜力，为其在LLMs领域及其他领域的广泛应用提供了有力支持。

定理4.1的证明。假设优化问题是可实现的，我们有如下关系：对于 t = 1,2,...，有 πt+1(y|x)∝πt(y|x) exp( ηP(y≻πt|x))。为了证明指数权重更新可以导致最优策略，我们直接引用 Freund 和 Schapire（1999）中定理1的重新表述版本：引理A.1（Freund 和 Schapire（1999）中的定理1，重新表述）。对于任何 oracle P 和任何混合策略序列 µ1, µ2,..., µT，由(A.1)产生的策略序列π1, π2,..., πT满足：TX t=1P(πt≺µt)≤min πη 1−e−ηTX t=1P(π≺µt) +KL(π∥π0) 1−e−η。

通过设定µt=πt，我们可以得出T 2≤min πηT 1 − e−ηP(π≺¯πT) + KL(π∥π0) 1 − e−η，其中不等式左边来自于P(πt≺πt) = 1/2，而不等式右边来自于1 TPT t=1P(π≺ πt) = P(π≺¯πt)。现在重新排列项可以得到1 − e−η 2η ≤min π P(π≺¯πT) + KL(π∥π0) ηT 。

我们可以天真地限定KL-散度KL(π∥π0)≤ ∥logπ0(·)∥∞，这可以看作是一个（较大的）常数。

通过选择 η=∥logπ0(·)∥∞√ T，我们有  1 2−s ∥logπ0(·)∥∞ 4√ T+O(T−1)≤min π P(π≺¯πT) +s ∥logπ0(·)∥∞√ T，其中左侧来自于泰勒展开1−e−η 2η=1 2−η 4+o(η)。请注意，左侧的 1 /2 已经是对称两人常和博弈的值。这表明，通过适当选择 η 和 T，混合策略 ¯ πT 接近于极小极大优化策略（纳什均衡）。

因此，最优性差距受 max π P(π≻¯πT) − min π P(π≺¯πT) 的限制。这里，max 表示找到最大值，min 表示找到最小值。式子可简化为 max π 1−P(π≺¯πT) − min π P(π≺¯πT) = 2(1− 1/2*min π P(π≺¯πT))，其结果为 O(1/√T)。

表4：我们的微调模型在不同迭代中由SPPO生成的示例。

给您一个任务的定义，然后是任务的一些输入。您将看到2名或更多人之间的对话。对话中会包括有关其中2名参与者关系的信息。最后，您将被要求确定这2名参与者之间的关系。在这种情况下，‘0’代表配偶关系，‘1’代表兄弟姐妹关系，‘2’代表父母子女关系。说话者1：我马上就来，好吗？谢谢，Wendy。说话者2：天哪！值班医生有多可爱啊？说话者3：哦，太可爱了，我都想把这支笔戳进自己的眼睛了。说话者4：伯克医生现在会见你。说话者2：哦不-不-不，不是伯克医生。伯克医生出城了。值班医生现在会见我。说话者4：理查德·伯克医生出城了。提莫西·伯克医生，他的儿子，现在会见你。说话者5：准备好了吗？说话者5和理查德·伯克之间的关系是父子关系，因为对话中另一位角色被确认为理查德·伯克的儿子。

第一次讲述中，Speaker 5和Richard Burke之间的关系是父子关系。根据对话内容，揭示了Timothy Burke博士是值班医生，也是Richard Burke医生的儿子。

第二轮交谈中，发言者5和理查德·伯克之间的关系是父子关系。对话透露出蒂莫西·伯克医生是现场医生，并且被认定为理查德·伯克医生的儿子。

在第三轮对话中，发言的第五个人是询问与某个关系的人，所以他们与理查德·伯克博士没有直接的关系。对话中提供的信息显示，值班医生是蒂莫西·伯克医生，他是理查德·伯克博士的儿子。

因此，答案是：说话者 5 和 Richard Burke 博士之间的关系是父子关系（Richard Burke 是 Timothy Burke 博士的父亲）。说话者 5 正在打探这段关系，但他们并不是其中的一员。所指定的关系代码（0 代表配偶，1 代表兄弟姐妹，2 代表父子母子）与这一描述相符：说话者 5（0、1 或 2）和 Richard Burke 博士（2）联系在一起，形成了一个父子关系。输出结果为：2（父子关系）在这种情况下，说话者 5 的作用是确定她与 Richard Burke 博士之间的关系，但由于他们没有亲缘关系（她是外部询问者），因此正确答案只与对话中确定的关系有关：Richard Burke 博士（父亲）和 Timothy Burke 博士（儿子）。

表格5：在不同迭代次数下，我们的精调模型再一代示例，由SPPO提供。

在NBC的肥皂剧《我们生活的日子》中，罗曼·布雷迪是一个虚构的角色。这个角色最早由韦恩·诺斯罗普在1981年到1984年间饰演。后来，德雷克·霍杰斯廷接替他，从1986年到1991年担任该角色。诺斯罗普在1991年到1994年再次回归。自1997年起，这个角色由乔什·泰勒出演。罗曼·布雷迪是该剧长期故事中的重要角色。

第一版：罗曼·布雷迪是美国全国广播公司肥皂剧《我们生活的日子》中的虚构角色。他首次由韦恩·诺思罗普在1981年至1984年间扮演。德雷克·霍格斯丁接替该角色，在1986年至1991年期间出演。诺思罗普于1991年至1994年间再次扮演了罗曼。自1997年以来，由乔希·泰勒出演该角色。

总的来说，罗曼·布雷迪是《我们生活的日子》中一个长期存在的角色，多年来由三位不同的演员扮演过：韦恩·诺斯罗普(1981-1984, 1991-1994)、德雷克·霍吉斯汀(1986-1991)，目前由乔希·泰勒(1997至今)扮演。

迭代2：罗曼·布雷迪是NBC肥皂剧《生活时光》中的虚构人物。他最初由韦恩·诺思罗普在1981年至1984年间饰演。德雷克·霍杰斯坦在1986年至1991年接替了这个角色。诺思罗普于1991年至1994年间再次出演罗曼。自1997年起，由乔希·泰勒扮演该角色。

总的来说，在这个节目的历史上，Roman Brady是一个历史悠久且复杂的角色，有多位演员为这个角色带来了各自独特的诠释。

《我们生活的日子》是一部由美国全国广播公司（NBC）播出的肥皂剧，其中的虚构人物 Roman Brady 在剧中扮演重要角色。他最初由 Wayne Northrop 饰演，时间是从1981年到1984年。之后，Drake Hogestyn 在1986年至1991年间接替扮演这个角色。Northrop 在1991年到1994年间再次回归，继续出演 Roman 的角色。自1997年以来，这个角色由 Josh Taylor 饰演。

Roman Brady 是这部长寿日间剧中一个复杂而突出的人物。早在80年代初，由Northrop出演时，他参与了各种情节，如感情关系和商业交易。Hogestyn 在80年代末和90年代初的扮演为这个角色引入了新的维度。Northrop在1991年至1994年的第二次出演进一步发展了Roman的故事。自1997年以来，Josh Taylor 承担了这个角色，继续为Roman Brady的故事增添新的元素。在这些不同的演绎中，Roman 仍然是一个中心人物，以他错综复杂的关系、商业冒险和个人挣扎而闻名。
ActiveClean: 通过主动学习生成线级漏洞数据
作者：Iowa州立大学的Ashwin Kallingal Joshy（ashwinkj@iastate.edu）、Mirza Sanjida Alam（sanjida@iastate.edu）、Shaila Sharmin（ssharmin@iastate.edu）、Qi Li（qli@iastate.edu）、Wei Le（weile@iastate.edu）
摘要：深度学习漏洞检测工具越来越受欢迎，并已被证明有效。这些工具依赖于大量高质量的训练数据，而这些数据很难获取。目前大多数可用的数据集提供功能级标签，报告功能是否存在漏洞。

然而，要使漏洞检测有用，我们还需要了解与漏洞相关的代码行。这篇论文努力发展系统化工具，并提出了ActiveClean 来从代码提交中生成大量的代码行级漏洞数据。也就是说，除了功能级标签外，它还报告了函数中哪些代码行可能与漏洞检测有关。过去，静态分析已被应用于清理代码提交以生成代码行级数据。我们的方法基于主动学习，易于使用且可扩展，提供了与静态分析互补的方法。我们从提交代码行中设计了语义和句法属性，并用它们来训练模型。我们在处理超过4.3K个提交和119K个提交代码行的Java和C数据集上评估了我们的方法。ActiveClean取得了70-74之间的F1得分。此外，我们还展示，仅使用400个训练数据就可以达到70.23的F1得分，证明了主动学习的有效性。利用ActiveClean，我们为Devign数据集中的整个FFMpeg项目生成了代码行级标签，包括5K个函数，并发现了不正确的功能级标签。我们证明了，使用我们清理过的数据，LineVul（一种先进的代码行级漏洞检测工具）检测到了70个更多有漏洞的代码行和18个更多的有漏洞函数，并将Top 10准确度从66%提高到73%。

1. 引言：基于深度学习的漏洞检测已被证明具有有效性，并正在成为减轻野外利用漏洞数量不断增加的重要手段。尽管漏洞检测模型正在快速发展，但获取高质量数据始终是一个挑战。目前的漏洞数据集中存在错误标签。大多数数据集使用函数级标签，并报告函数是否存在漏洞。理想情况下，工具应报告行级漏洞，即哪些行导致漏洞。缺乏高质量的行级漏洞数据集，构建行级漏洞检测模型可能会面临重大挑战。本文评估部分显示了，通过使用改进的数据集训练，行级模型的性能可以得到提升。

在过去，大多数漏洞检测数据集，如Devign、MSR/Big-Vul和D2A等，都是从代码仓库的软件补丁中获取的。例如，Devign使用提交消息中的关键字，然后进行手动检查以生成函数级别的标签。虽然我们需要大量示例来训练漏洞检测模型，但手动清理这些补丁以生成行级别的标签非常昂贵。最近的一项工作中，为28个Java项目和3,546个提交生成行级别标签，需要45位作者和超过6个月的时间。D2A使用静态分析工具在提交之前和之后生成的警告差异来生成行级别数据集。虽然这种方法看起来很有前途，但它依赖于静态分析工具准确检测漏洞的能力。

在这项工作中，我们尝试自动生成软件漏洞数据的一种新方法。我们开发了一个名为ActiveClean的工具，它利用主动学习来训练机器学习模型来清理软件补丁。我们选择了主动学习方法，因为手动标记提交行是昂贵的，而主动学习可以帮助选择最佳且最少数量的数据点进行标记。我们采用基于机器学习的方法，因为训练好的模型易于使用，并且可扩展处理漏洞检测所需的大量数据。

ActiveClean接收包含易受攻击提交的程序，并报告一行是否与漏洞相关。ActiveClean还可以接收现有的基于函数级别的漏洞数据集，并返回基于行级别的标签。在这种情况下，如果发现函数中的所有行都被标记为与漏洞无关，则ActiveClean还可以报告函数级标签不正确。

在ActiveClean中训练模型时，我们开发了一个工具，用于自动提取提交行中的句法和语义特征。我们设计的特征不仅展示了提交行本身的特征，还展示了它们与其他提交行以及周围代码的互动方式。然后我们使用基于委员会查询的主动学习框架，首先从一小部分现有的基于行级漏洞数据集中训练一组委员会模型。在主动学习的第二阶段，我们向委员会发出查询以选择最佳数据进行标记，以继续在新输入数据上训练模型。

在我们的评估中，我们使用了Java和C数据集，共处理了4375个提交，包含了11.9万行提交代码。我们的研究结果显示，在具有基准真相数据的数据集中，ActiveClean能够清理提交信息，并减少与漏洞无关的代码行数，F1 分数在 70-74 之间。这项研究于2023年12月由Ashwin Kallingal Joshy、Mirza Sanjida Alam、Shaila Sharmin、Qi Li 和 Wei Le在美国艾姆斯的ArXiv（一个学术预印本平台）发表。

在消融研究中，ActiveClean始终表现优异，胜过基准模型和不同设置。我们还展示了主动学习确实提高了效率。要达到70.23的F1分数，ActiveClean只需额外400个训练数据，而基准模型则需要2千个训练数据。在添加2千个额外训练数据后，ActiveClean可达到70.23的F1分数，而基准模型的F1分数仅达到67。

我们利用ActiveClean在Devign数据集中生成了FFmpeg的逐行漏洞数据。通过ActiveClean的帮助，我们还发现了现有Devign数据集中不正确的函数级标签。我们随机抽取了ActiveClean报告的468个错误标签中的50个样本，发现其中29个确实不是漏洞，还有8个无法在手动确认后证实。通过我们生成的清理数据集，最先进的漏洞检测模型LineVul 检测到了18个更多的易受攻击函数和70个更多的易受攻击代码行。

在功能级别检测方面，它达到了87的F1分数；在行级别检测方面，达到了73%的Top 10准确率。而与使用原始数据集时达到的83的F1分数和66%的Top 10准确率相比，有了显著的提升。

总的来说，本文提出了一种自动且可扩展的工具，用于生成基于代码行的漏洞数据。尽管我们还没有达到100%的准确率和召回率，但我们证明了使用我们清理过的数据，漏洞检测模型的性能可以得到提升。我们希望通过为FFmpeg等提供基于行级标签的数据集，可以推动深度学习在漏洞检测领域的更好工具的发展。与此同时，我们也看到许多其他应用，比如自动修复和故障定位，可以从清理过的补丁中受益。未来，我们将继续使用ActiveClean生成更多类似的数据集。

我们的论文贡献包括：（1）一种自动技术和可扩展工具，可以清理代码补丁并生成基于代码行级别的漏洞数据，（2）具有代码行级标签的FFmpeg漏洞数据集，包含5千个漏洞函数和9千个标签，（3）特征工程和主动学习的应用作为这一问题的一种新方法，（4）系统性评估表明我们的方法可以提高数据集质量，并改进用于漏洞检测的深度学习模型。

一个激励人心的例子是Defect4J [24]，它是一个广泛使用的基准测试，其中修补程序被手动最小化，以包含对修复错误所需的最小更改。

然而，由于清理提交的挑战，Defect4J中的补丁有时仍然包含与错误无关的更改[48]。在图1中，我们展示了一个未经清理的补丁示例，该补丁已从Defect4J的Math 65中适配而来。

这个补丁影响了两个函数：在第2行的getRMS函数和在第12行的getChiSquare函数。这次提交包括12行代码（第3到第9行，第14、15、18、21和22行）。这12行中，只有第18行是用来修复第19行可能出现的除零错误。在getRMS中的改动是代码重构，也就是开发者创建了getChiSquare来替代第3到第7行被删除的代码。同样，在getChiSquare中，有另一个一段 @@ -239，9 +239，5 @@ 的改动：在第239行修改了代码2 public double getRMS() { 3- double criterion =0; 4- for ( int i = 0; i < rows ; ++i) { 5- final double residual = residuals [i]; 6- criterion += residual * residual / residualsWeights [i]; 7- } 8- return Math.sqrt( criterion / rows ); 9+ return Math.sqrt( getChiSquare() / rows ); 10 } 11@@ -254，11 +254，11 @@ 12 public double getChiSquare() { 13 double chiSquare = 0; 14- double sqHalf = 0; 15+ double chiSqHalf = 0; 16 for ( int i = 0; i < rows ; ++i) { 17 final double residual = residuals [i]; 18+ if residualsWeights [i] != 0 19 chiSquare += residual * residual / residualsWeights [i]; 20 } 21- sqHalf = chiSquare / 2; 22+ chiSqHalf = chiSquare / 2; 23 return chiSquare ; 24 } 图1：一个来自于Defect4J Math 65的例子，其中进行了重构，将变量sqHalf重命名为chiSqHalf。请查看第14、15、21和22行。

在这个例子中，getRMS函数中的补丁完全与漏洞无关。如果将这样的补丁用作漏洞检测数据，我们将给该函数引入一个错误的标签。即使是forgetChiSquare函数，提交中的大部分代码行也与bug无关。使用这样的提交创建基于代码行的漏洞数据可能会产生太多噪音。同样，未经清理的提交也可能对其他应用程序（如自动程序修复）造成威胁。

例如，来自Defect4J的Math 65的原始补丁导致先进的APR工具CapGen将其生成的一个正确的补丁错误地标记为不正确，原因是没有“修复”getRMS函数。

而且，这也导致[40]对于修复错误所需的更改类型进行了错误标记。

我们的方法旨在利用代码提交的句法和语义特征以及提交周围的代码来区分一行代码与缺陷是否相关。我们希望利用机器学习模型从已知有地面实况标签的现有提交中学习模式，并使用这些模型来预测提交中的某行代码是与缺陷相关还是不相关。例如，对于图1中的情况，修复漏洞的代码（第18行）引入了一个ifguard条件，涉及在ifblock中使用的一个变量的非零检查。

以前的研究显示，这种修改通常用来修复漏洞。

另一方面，在第14、15、21和22行将 sqHalf 更改为 tochiSqHalf 的变化中，我们可以看到没有控制流程的变化。同时，在整个补丁和整个函数或文件中，也没有与其他变量的数据依赖关系发生变化。这是重构中常见的模式。对于 getRMS 函数（第3-9行）的重构，我们可以观察到删除的变量 criterion 在 forActiveClean: Generating Line-Level Vulnerability Data via Active Learning ArXiv, December 2023, Ames, USA 循环（第4-7行）中被使用，进行了一些算术运算，并最终在第8行作为 Math.sqrt 函数的输入。与此同时，在补丁之后，Math.sqrt 函数的输入变为一个函数（getChiSquare）。

这种类型的模式可以帮助识别与漏洞无关的代码行。

我们训练了一个主动学习模型，根据提取的特征，对提交的每一行进行分类，判断是否与漏洞相关。然后我们在函数级别的漏洞数据上使用该模型，生成了每行漏洞数据。

3 方法 3.1 概述 图2展示了我们工作的概述。图2a显示了ActiveClean的输入是包含其易受攻击提交的程序。

输出的是行级漏洞数据。ActiveClean 还可以处理带有提交信息的函数级别漏洞数据。在这种情况下，ActiveClean 将清理提交信息并生成行级标签，同时查找可能存在于函数级别标签中的错误并建议标签更正。

为了实现目标，ActiveClean通过一个主动学习框架训练了一个机器学习模型。具体请参考图2b。这个主动学习框架使用多个机器学习模型来组成一个委员会。训练从一小部分带标签的数据开始。这些带标签的数据包括少量的代码行漏洞数据。它们的标签标记了提交中的一行代码是否与bug相关。一旦模型训练完成，主动学习就开始了。委员会指导我们应该为未标记数据集中的哪些提交行加标签，以便这个数据集可以快速达到最佳性能。用户接着为委员会推荐的提交行生成标签。这些新加标签的数据被用来训练和改进模型。这个过程迭代进行，直到达到一个预定的预算，比如时间，或者达到期望的准确性。在应用到一个新的数据集时，为了进一步提高性能，ActiveClean可以继续用新数据集中一小部分未标记数据进行训练。

一旦活跃学习模型训练完毕，我们就将最终模型应用于图2a所示的输入，以获取基于代码行的漏洞数据。如果我们观察到某个函数中有异常数量的代码行被标记为非易受攻击的，我们将向用户报告该函数，建议他们进一步检查，以及可能存在将非易受攻击的函数错误标记为易受攻击的情况。用户可以配置阈值以定义在其项目中何为异常情况。

3.2特征工程 当手动清理一些补丁时，代码检查员可能并不总是能够完全理解代码的详细功能 [11, 20, 48]。 从句法和语义代码特征中得出的模式可能提供线索，以确定代码是否与错误相关。 例如，添加条件保护或涉及块内变量的 try-catch 块，如图1所示，可能表明包裹的块内的代码存在错误。

我们的目标是利用机器学习，通过从提交中提取的特征以及提交行周围的代码上下文来学习模式，从而自动区分提交行是否与漏洞相关。我们研究了来自手动最小化的 Java 数据集[19]以及过往研究[22,34]中的250个提交，以确定一组可能有助于区分提交中漏洞相关行和漏洞不相关行的模式和属性。

在表1中，我们展示了我们设计的特征。这些特征可以分为三类：（1）捕捉每个单独提交行本身特征的特征（显示在提交行下），（2）捕捉不同提交行之间如何相互影响的特征（在提交内部），最后（3） 捕捉提交行如何与其他代码互动的特征（在提交上下文下）。

Commit Lines 功能背后的直觉是识别在代码行级别可能存在的简单错误模式。例如，在提交行下的表1中展示的比较器、算术和逻辑功能捕获了提交行中存在的每种运算符类型的数量。经常出现的一种错误是 off-by-one 错误，修复这种错误可以通过改变其中一种运算符的类型或数量来解决，并且可以通过它们捕获到。举个例子，常见的 bug 修复通常涉及“从函数中返回并报告错误消息”。这可以使用 hasRet 捕获，它检查代码行中是否包含返回语句，结合 hasLiteral，它检查语句中的字符串来实现。同样，使用布尔变量来避免错误的控制流程可以通过 flagVar 功能来捕获。

「Within The Commit」中的特征被设计用来区分功能增强和重构的变化类型与错误修复。例如，大多数错误修复只会最小限度地改变其控制和数据依赖关系。另一方面，功能增强和重构引入了更多的控制和数据依赖关系，无论是在提交内部还是周围。这可以通过控制依赖和依赖特征的变化（见表1）来捕捉，该特征报告了控制和数据依赖的数量。像是 repeated、repeatedCall 和 repeatedControl 这样的特征被设计用来捕获重构变化中常见的模式，其中相同的代码或函数调用在提交中反复出现。

提交上下文记录了提交的代码行的位置，以及它们与程序的交互方式。它们为确定代码是否与 bug 相关提供了宝贵线索。根据我们的观察，大多数 bug 修复通常发生在现有的控制、功能或循环体内。控制块和其他基于块的功能（请参见表1）记录了更改是否发生在其名称对应的体内。

在软件功能增强和重构改变中，通常会引入新的函数和变量。这些新引入的函数和变量之间的控制和数据依赖关系要比与其周围环境的依赖关系更多。基于块的功能以及依赖于、受控于等功能会检查与周围环境的控制和数据依赖关系，以便捕捉这些情况。

我们的目标是设计与底层实现语言和项目无关的功能。因此，我们不使用任何实际的代码，以避免学习特定项目的函数或变量名称，并使用常规语义模式而不是更详细的语言依赖模式。我们还将它们设计成易于机器学习消费，只使用基于布尔值和数字的特性。在这项工作中，我们使用代码属性图（CPG），它整合了控制流图、抽象语法树和依赖图，以表示提交的源代码并提取特征。

具体来说，我们在Joern[1]中使用了它的实现，这是一个代码分析平台，支持包括C、C ++、Java和Python在内的多种编程语言。

为了提取特征，我们针对每个提交，提取了提交中的所有源代码文件的当前版本和补丁前的版本。然后我们使用Joern为它们构建了CPG。接下来，我们使用Joern分析了每个单独提交行的语法和语义属性，并使用我们设计的自定义CPG查询提取了表1中列出的特征。3.3 应用主动学习来清理提交我们将补丁清理问题形式化为一个机器学习分类问题。这里的输入是代表提交行特征的向量。输出是一个标签，“1”表示该行与漏洞相关；“0”表示该行是其他类型的更改，例如重构或功能增强。

我们采用了积极学习的方法，具体来说是通过委员会提问的方式来学习从已提取特征中识别出与错误相关的提交行的模式。委员会提问是一种非常有效的积极学习方法，已成功应用于不同的分类问题[33]。具体而言，我们使用了评估度量标准为投票熵的委员会提问方法ActiveClean：通过积极学习生成基于行级漏洞数据的ArXiv提交，文章发表于2023年12月，地点在美国艾姆斯，采用modAL[13]，这是建立在Scikit-learn[35]之上的机器学习框架。

一个委员会可以由两种或更多传统的机器学习方法组成，比如随机森林或支持向量机。

在给定一组未标记数据的情况下，查询委员会通过迭代方式选择要标记为训练数据的数据。通常情况下，选择是通过委员会对预测标签存在分歧的某种度量来确定的。委员会通常选择最不确定且具有最大分歧的示例，因为标记这些示例可以为模型带来最大的信息量。

使用这种方法，主动学习通常只需要更少的标签，就能快速学习出一个好的模型。

在我们的方法中，首先，我们利用提取的特征以及已知的行级标签来训练一组用于委员会的初始基础模型。接下来，我们从没有行级标签数据的提交行中提取特征。然后，我们利用不同委员会模型之间的预测差异来选择在每次迭代中标记固定数量的额外数据。用户手动检查并为这些提交行提供标签。最后，我们重复查询和标记，直到达到固定的预算，例如按时间或可标记的数据数量来衡量。

如果输入模型的是具有漏洞提交的程序(如图2a所示)，我们使用主动学习后的最终模型来预测每个单独提交的代码行是否与漏洞相关。根据每行的预测结果，将提交的代码行编码为“1”或“0”，具体取决于它们是否与漏洞相关，作为输出。

在函数级别的漏洞数据中，除了输出每行的数据外，我们还会输出存在漏洞的函数中漏洞行的行号，以及由于非漏洞行的异常数量而需重新检查存在漏洞的函数的输出信息。

评估 在本文中，我们旨在回答以下研究问题： •RQ1 [验证] 我们的方法能否有效和高效地清理出错误提交？ •RQ2 [比较] ActiveClean与其他基线和设置相比如何？ •RQ3 [应用] 我们的方法如何帮助深度学习漏洞检测？ 4.1 实验设置 4.1.1 实施 我们使用Joern [1]，Scikit-learn [35]，modAL [13]，Python，Bash和Scala为C和Java程序实施了ActiveClean。具体来说，我们使用Joern与Bash脚本和Scala从补丁中提取特征。然后我们使用Scikit-learn和modAL进行主动学习。

4.1.2 主题选择。为了回答研究问题并展示我们的技术在实践中的适用性，我们的目标是使用符合以下条件的基准测试：(1) 这些基准测试是Java和C的真实世界开源程序，(2) 这些基准测试具有手动标记或验证的错误修复提交，这样我们就可以有基准真相进行比较，(3) 这些基准测试在研究社区中被积极使用，以便我们的结果可以直接惠及用户。我们在文献中搜索符合上述标准的现有基准测试 [5,7,8,12,14,15,19,20,23–25,28,32,42,45,48,51,53]。结果，我们选择了[19]中提供的所有17个Apache项目作为Java程序的基准测试，因为它提供了手动验证的错误修复提交的行级基准真实标签。这给我们提供了365个提交中分布的27,000个行级标签。然而，我们未能找到C程序的相应基准。虽然像SIR [15]、ManyBugs [28]和DBGBench [5]这样的基准有所希望，但SIR只包含了有意引入的错误。而ManyBugs和DBGBench每个程序报告的错误非常少。

因此，在我们的研究中，我们使用了来自Devign的FFmpeg和QEMU[53]。数据集包含函数级漏洞标签，被广泛引用。本文两位作者手动检查了提交，并按照文献中记录的手动检查协议为一小部分数据提供了行级标签，以满足我们评估的需求。在21天的预算内，两位作者为260个（170个FFmpeg + 100个QEMU）随机选取的提交提供了标签。即使在作者之间讨论后，这260个提交中有13个（3个FFmpeg + 10个 QEMU）的标签存在分歧。因此，我们将它们从我们的数据集中排除。总体上，我们为257个（167个FFmpeg + 90个QEMU）用于C代码的提交生成了4.4K（3.2K个FFmpeg + 1.2K个QEMU）行标签。

4.1.3 选择机器学习模型建立主动学习框架。我们采用基于“委员会查询”[13]的主动学习框架来训练模型。该委员会包括两种模型。为了选择这些模型，我们考虑了五种著名的机器学习模型，分别是随机森林[4]、标签扩散[52]、标签传播[54]、支持向量机[9]和逻辑回归[10]。我们使用了来自[ 19]中另外两个Java项目spike和giraph的真实标签，来选择这个委员会。基于这项初步分析，ActiveClean被实现为一个以随机森林和支持向量机为基础的“委员会查询”主动学习模型。

4.1.4 RQ1的实验设计。在RQ1中，我们的目标是评估使用ActiveClean生成的标签的效果和效率。我们通过评估减少提交次数和减少的正确性来衡量效果。具体来说，对于提交次数的减少，我们衡量(1)平均删除的与错误无关的代码行数，以及(2)删除了其错误无关代码行的提交次数。对于减少的正确性，我们报告了预测的F1分数。对于效率，我们报告了通过ActiveClean实现的F1分数所需的训练示例数量。

在Java数据集上，我们使用基准[19]提供的基准真实标签来评估我们的方法的有效性和效率，采用3倍交叉验证。我们的设置遵循主动学习文献[20,50]。首先，我们使用训练数据的20%来训练ActiveClean，建立委员会中的基础模型。第二步，我们从剩余的80%训练数据中向委员会查询200次并每次交互查询10条提交线。在训练额外的2K提交线之后，我们绘制了F1分数的查询间隔图，以评估学习效率。最后，我们使用在测试数据上进行的预测结果来收集有效性指标。

为了为C项目训练基础模型，我们使用了整个Java数据集。这是因为我们只能标记少量的C数据。在第二步，我们向委员会查询了100次，每次查询使用10行提交数据（来自FFmpeg）。这1K行提交数据的标签是由一位作者根据需要生成的。训练完成后，我们绘制了查询间隔处的F1分数，以评估效率的提高。ArXiv，2023年12月，美国艾姆斯，作者为Ashwin Kallingal Joshy, Mirza Sanjida Alam, Shaila Sharmin, Qi Li和Wei Le，他们都被用作测试数据。

最后，为了评估效果，我们使用了在测试数据上进行的预测，以及剩下的未标记的FFmpeg数据。

因为我们对未标记的 FFmpeg 数据没有真实的参考数据，所以我们随机选择了 500 行提交记录（分别来自漏洞修复和与错误无关的预测，每部分 250 行），并按照我们上面提到的手动检查协议进行了人工评估。

最后，我们想要评估ActiveClean在新项目中的有效性。具体来说，我们想要衡量ActiveClean在一个未知的C语言项目上的表现，以及我们是否能够快速地将ActiveClean适应到新项目中去。在这个实验中，我们使用了QEMU，这是Devign中另一个C语言数据集。两位作者按照以上给定的手动检查协议，手动检查和标记了100个提交。由于意见不一致，我们放弃了10个提交的标签。我们创建了 2.8K 标签，并将这个带标签的数据集用作测试集。然后，我们将在Java和FFmpeg数据集上训练过的ActiveClean应用于QEMU测试集。性能是用F1分数来衡量的。接着，为了衡量适应性，我们使用“查询委员会”方法来查询并教授100个额外的标签。我们使用F1分数的变化来衡量适应性。

实验设计用于研究问题2。为了比较我们使用的主动学习方法，我们选择了排名前3的机器学习模型作为基准线，即标签传播(Label Spreading)、随机森林(Random Forest)和支持向量机(Support Vector Machine)。我们选择这些模型是因为它们在第4.1.3节中的spike和giraph数据集中排名靠前。我们对每个模型进行了200次训练，从使用20%的训练数据开始，然后每次添加10个训练数据。这有助于我们了解这些模型需要多少最少数据才能达到最佳性能，以便与主动学习进行比较。我们还查阅了文献，寻找其他可以清理基于行级提交的基准线[3,6,8,14,17,23,27,36,39,42,47]。

在我们能够运行的工具中，（1）FLEXME 只能处理 C# 项目；（2）LineVul 能够预测代码行级的漏洞，但由于我们的基准数据不在分布范围内，报告的 F1 值不到 15%。

在RQ2中，我们也进行了两项消融研究。在第一项研究中，我们旨在研究模型选择对查询委员会的影响。

为了实现这一点，我们使用了不同的查询委员会训练了3个模型。具体来说，我们联合使用了随机森林、逻辑回归、标签扩散和标签传播等查询委员会，因为它们在我们的调优数据集中表现最好（见第4.1.3节）。这些模型使用了与ActiveClean相同的初始训练数据进行训练。然后，我们使用它们的委员会从训练数据集的其余部分中选择了与ActiveClean相同数量的额外训练数据。

在第二项研究中，我们旨在了解将主动学习应用于有标签数据相比随机采样的优势。因此，我们使用相同的基础模型，用20%的训练数据进行训练。与使用主动学习不同的是，在每次迭代中，我们以跟ActiveClean训练一样的方式随机采样相同数量的数据。我们绘制了两种方法的F1分数随迭代次数变化的图表。在基准和消融实验中，我们使用了Java数据集，以确保可以使用大量的有标签数据。

4.1.6 RQ3的实验设计。RQ3旨在展示ActiveClean的实用性。在我们的第一个应用中，我们将清洗后的数据集应用于行级漏洞检测，并将模型性能与未经过我们清洗的数据进行比较。我们使用了LineVul [17]，这是目前仅能在FFmpeg上运行的行级漏洞检测模型。我们使用了来自FFmpeg的9.5K个函数对LineVul进行了训练。在普通设置中，我们将函数的提交作为行级标签，而在ActiveClean设置中，我们使用我们的清洗数据集。为了比较测试集的性能，我们使用F1分数、前10个命中精度和IFA（前10个中第一个有漏洞行的平均位置）这些指标来衡量预测的漏洞行与真实情况之间的准确性。

在第二个应用程序中，我们探讨了使用ActiveClean来建议标签更正的可能性。具体来说，我们建议对于任何具有超过50%漏洞不相关代码行的脆弱函数进行重新分类。我们随机抽取了ActiveClean提出的50个被认为是漏洞不相关的功能，然后进行了手动检查以确定它们的正确性。

由于ActiveClean可以纠正功能级别的标签，我们还分别使用来自Devign和来自ActiveClean的标签运行LineVul进行功能级别预测。我们使用F1分数来比较这两种设置的性能。

在使用 LineVul 进行实验时，我们对基准和 ActiveClean 使用了相同的测试集。这些测试集是来自 FFmpeg 的手动标记的真实数据。

运行实验。特征提取、ActiveClean 和基准模型在一台配置有 64 位、16 核 Intel Haswell 处理器和 32 GB RAM 的虚拟机上进行训练和评估。LineVul 模型在一台配置有 32 核 CPU 和 GPU、16 GB RAM 的虚拟机上进行评估。这两台虚拟机都运行在 CentOS 8 操作系统上。

第4.2节中展示了RQ1的结果：验证。表2中，我们展示了来自[19]的Java数据集标记的结果，手动标记的FFmpeg和Qemu在相应的列Java测试、FFmpeg测试和Qemu新项目测试中的结果。在FFmpeg应用程序下，我们报告了使用Devign处理整个FFmpeg数据集的结果（不包括在FFmpeg测试中手动标记的数据）。

在ActiveClean F1分数这一行，ActiveClean报告了FFmpeg测试的最高F1分数为74.83，并且报告了Java测试的F1分数为70.23。由于我们没有FFmpeg应用的行级标签，我们没有报告F1分数；然而，我们对结果进行了抽样以手动确认预测（详见后文）。

在申请 Qemu 时，我们没有经过任何重新训练，报告的 F1 分数为 59.53。在提供额外 100 行标签后，我们在 Qemu 新项目测试中获得了 64.46 分的 F1 分数。

ActiveClean 能够减少所有数据集中与错误无关的提交代码行，这一点在“总错误无关行（预测）”和“总错误无关行（确认）”中有所体现。对于 FFmpeg 应用程序，我们抽样检查了 500 条数据 （250 条与错误相关，250 条与错误无关）。在 ActiveClean 提供的 250 条与错误无关行中报告的数据中：通过 Active Learning ArXiv 在 2023 年 12 月在美国艾姆斯举行的“生成基于行级别漏洞数据的活跃学习”的研讨会上所展示的数据集 Java 测试 FFmpeg 测试 FFmpeg 应用程序 Qemu 新项目测试 ActiveClean F1 得分 70.23 74.83 — 64.46 总提交行数 8909 3129 98267 1200 总错误无关行数（预测） 6832 1202 51950 748 总错误无关行数（确认） 5792 544 185/249 419 总提交次数 364 167 5246 90 含有错误无关行的提交（预测） 342 114 3628 76 含有错误无关行的提交（确认） 332 57 — 43 平均提交大小 25.09 18.73 17.01 13.33 平均每次提交的错误无关行数（预测） 20.42 10.54 14.31 9.84 平均每次提交的错误无关行数（确认） 17.49 6.03 — 9.74 表2：RQ1 的结果：有效性 0 250 500 750 1000 1250 1500 1750 2000 查询次数6970F1 得分(a) Java 测试的 F1 得分 vs 标记数据数量为 2K 的额外训练数据 0 100 200 300 400 500 600 700 800 900 1000 查询次数7172737475F1 得分(b) FFmpeg 测试的 F1 得分 vs 标记数据数量为 1K 的额外训练数据 图3：RQ1 的结果：以 F1 得分为衡量标准的效率针对经过训练的标记数据数量 ActiveClean ，我们确认有 185 条被准确预测为与错误无关的行，而在与错误相关的行中，我们确认有 187 条中确实与错误相关。我们排除了来自不同观点的 8 条（一条与无关，七条与相关）中的数据以便进一步考虑，因为两位作者无法达成一致意见。Java 测试中的明显较多的错误无关行数是由于该数据集中 bug 修复提交中存在的文档和测试用例更改导致的。

ActiveClean还表明，与Bug无关的代码行分布在所有数据集的提交记录中，如“具有Bug无关行（预测）”和“具有Bug无关行（已确认）”栏所示。因此，像ActiveClean这样的基于代码行级别的清理工具对于许多提交记录都可能很有用。即使是针对除有关文档和测试之外（不像Java测试），专注于漏洞函数的FFmpeg测试和Qemu新项目测试这类数据集，我们也报告了需要进行清理的许多提交记录。

每次提交中与错误无关的代码行数的普遍情况显示在“平均每次提交中与错误无关的代码行数（预测）”和“平均每次提交中与错误无关的代码行数（确认）”这两行中，而提交的平均规模显示在“平均提交大小”里。

在Java测试数据集中，一次提交平均有25行代码，其中大约有17.5行与bug无关。有趣的是，Qemu新项目的测试报告显示，每次提交平均有超过9行与bug无关，而平均代码量仅约为13行。这表明平均而言，有很大一部分提交需要进行清理，因为它们与漏洞无关。直接使用提交作为代码级别的标签可能会为代码级别的漏洞检测带来大量噪音。

图3显示了ActiveClean效率的结果。 Y轴为F1得分。 X轴显示了在主动学习期间提供的标记行数。图3a和图3b分别显示了Java测试和FFmpeg测试的对应图表。

实验结果显示，ActiveClean 能够快速提高两个数据集的 F1 分数。图 3a 显示，为了使 Java 测试的 F1 分数达到 70，ActiveClean 只需要额外增加 400 行提交代码。同样地，如图 3b 所示，对 FFmpeg 测试再增加 400 个训练数据，就能使 F1 分数从 71.5 提高到 74。

为了全面了解效率情况，我们使用了所有可用的训练数据（共 6.6K 个标签）对 Java 测试进行了模型训练。这个模型的 F1 分数为 72.82。然而，值得一提的是，ActiveClean 只需要额外增加 400 条数据（在基础模型的 20% 基础上），就能接近最佳表现水平。

RQ2问题的结果：基准模型比较平均F1分数。ActiveClean获得了70.23的最佳平均F1分数，表现最好。与之相比，Label Spreading，Random Forest和Support Vector Machine的平均F1分数依次为67.18，67.17和67.07，在最终训练后这些基准模型表现相似。

在图4中，我们绘制了Label Spreading（显示为蓝色）在不同额外训练数据量下的F1分数，它是最佳基准线。我们还为了比较绘制了ActiveClean的F1分数（显示为橙色）。我们可以看到，在最初的训练后，基于委员会的ActiveClean的F1分数达到了68.56，而Label Spreading模型的为66.68。这种初步领先优势通过学习委员会建议的数据很快得到提高。ActiveClean仅需额外50个数据就可以达到69，再额外400个数据就能达到70。与此同时，即使再额外2K个训练数据，Label Spreading也无法提高一个完整的分数。这清楚地展示了主动学习的优势，它可以在较少训练数据的情况下快速学习。

我们在表4中呈现了两项消融研究的结果。在第一个研究中，我们尝试使用不同的委员会进行主动学习。ActiveClean的F1分数为70.23，紧随其后的是Random Forest + Logistic Regression模型，得分为69.10。标签传播和标签扩散的模型表现相似，分别为68.812和68.765。然而，所有基于不同委员会的模型表现均优于基准模型。

这个研究中各种模型的平均 F1 得分如下：ActiveClean 70.230；消融研究：主动学习随机森林 + 逻辑回归 69.108；随机森林 + 标签传播 68.812；随机森林 + 标签扩散 68.765；消融研究：随机选择随机森林 + 支持向量机 69.031。表格4展示了RQ2的结果：消融研究：在最终训练后的平均 F1 分数。本研究的效率比较显示在图5a中。

在ActiveClean不同数量的额外训练数据下，F1分数以橙色显示；而基于Random Forest + Logistic Regression的下一个最佳设置以蓝色显示。初始训练后，ActiveClean的F1分数为68.54，而Random Forest + Logistic Regression为67.74。训练额外的120个提交行数据后，ActiveClean的F1分数提高了1点至69.58。与此同时，Random Forest + Logistic Regression需要额外训练240个数据，即原来的两倍，才能提高1点至68.74。总体而言，所有基于委员会的模型都呈现稳步改善，但增长速度较慢。例如，采用Random Forest + Label Propagation模型的F1分数需要额外1000个训练数据才能达到69分。

在第二次消融研究中使用随机选择方式时，与ActiveClean的F1分数70.23相比，在额外的2K训练数据结束时，该模型的F1分数为69.03。我们发现，使用ActiveClean的委员会进行随机选择仍然优于其他委员会选择方式。图5b显示了效率比较，其中ActiveClean（橙色）在每增加一条训练数据时，F1分数的提高速度均快于使用随机选择进行训练的同一委员会模型（以ActiveClean-Random蓝色显示）。
两个模型在初始训练后的F1分数都为68.54。但是，ActiveClean仅用了额外的50条训练数据就超过了69的F1分数，而ActiveClean-Random则需要500条。虽然ActiveClean最终达到了70.21的F1分数，但ActiveClean-Random只用2K额外数据达到了69.341的F1分数。

RQ3的结果：应用模型 F1 值前十位IFA正确预测功能行数

ActiveClean-FFmpeg模型的结果如下：准确预测功能行数为87，准确率为73%，IFA为6.68，活动数据生成的功能行数量为238。FFmpeg模型的结果为：准确预测功能行数为83，准确率为66%，IFA为8.11，活动数据生成的功能行数量为220。

RQ3结果表格如下：使用清洁数据集运行的LineVul

《ActiveClean：通过主动学习生成行级漏洞数据》

ArXiv，2023年12月，美国艾姆斯

附加训练数据数量与F1得分

活动学习消融研究：随机森林+逻辑回归（第二佳设置）与ActiveClean比较

《ActiveClean-Random：随机选择的基于ActiveClean的活动学习消融研究》

RQ2结果图：效率与额外训练数据量之间的F1值比较

在行级别上，使用清洁数据集ActiveClean-FFmpeg，LineVul报告了178个易受攻击的行，而原始数据集FFmpeg仅报告了108个易受攻击的行。这是1.6倍的改进，结果是额外70个易受攻击的行。ActiveClean-FFmpeg模型还报告了73%的Top 10准确率，而FFmpeg模型报告了66%。ActiveClean-FFmpeg模型的IFA为6.68，而FFmpeg模型的IFA为8.11，改进了1.43。IFA值越低表示模型排名前十位的与错误无关的代码行数越少。换句话说，开发人员通常需要查看比原来少1.5行才能找到易受攻击的行。

我们总共审查了50个函数，其中有29个函数不易受攻击，13个函数存在安全漏洞，8个函数暂未确定。在RQ3的结果表中显示，ActiveClean可以帮助修正函数级别的标签。在我们的实验中，ActiveClean为FFmpeg数据集推荐了468次标签更正。我们随机抽样了50个函数进行检查，两位作者一致认为其中29个函数，占69.04%，确实是不易受攻击的。

在本节中，我们将提供两个有趣的示例，这是ActiveClean推荐的不正确的函数级别标签。

在第6张图片中，我们展示了在FFmpeg中的一个例子，Devign为alac_decode_close（老版本）提供了一个错误的函数级别标签。图6中显示了diff patch1，红色表示删除，绿色表示添加。在这里，老版本函数的第17-22行被移动到了1-6行。这个函数不是1https://github.com/FFmpeg/FFmpeg/commit/53df079a730043cd0aa330c9aba7950034b1424f1+ static av_cold int alac_decode_close（AVCodecContext *avctx）{2+ALACContext *alac = avctx->priv_data;3+int chan;4+for（chan = 0; chan < alac->numchannels; chan ++）{5+av_freep（&alac->predicterror_buffer[chan]）;6+...

在这段代码中，我们看到了一个ALAC音频解码器的初始化函数。这个函数会接收一个AVCodecContext类型的参数，并且返回一个整型值。在函数内部，首先声明了一个整型变量ret和ALACContext结构体指针alac，并且将avctx对象的私有数据赋值给alac。接下来是函数的具体实现，这里省略了一些代码。

在这段代码中，函数 allocate_buffers(alac) 被调用来分配缓冲区。然后函数返回0。接下来是函数 alac_decode_close(AVCodecContext *avctx)，它被标记为静态冷启动函数。其中获取 AVCodecContext 结构体中的私有数据 alac，并对通道进行循环处理，直到循环结束为止。在循环中，通过 av_freep() 函数释放每个通道的 predicterror_buffer 缓冲区的内存。

23行：如果(尝试为ALAC分配缓冲区的返回值ret小于0) {
24行：记录错误消息"分配缓冲区时出错\n"到avctx中；
25行：返回ret；
26行：返回0；
27行：图6：FFmpeg中Devign数据集中一个错误的标签存在，存在漏洞且未在提交中添加对该函数的更正。ActiveClean正确地检测到了这一问题。

图7显示了一个修补程序，只是格式化了代码，但第1行的RENAME函数被标记为容易受攻击的，在Devign 2中（https://github.com/FFmpeg/FFmpeg/commit/6e42e6c4b410dbef8b593c2d796a5dad95f89ee4ArXiv，2023年12月，美国艾姆斯），Ashwin Kallingal Joshy、Mirza Sanjida Alam、Shaila Sharmin、Qi Li和Wei Le对static inline void RENAME(yuvPlanartoyuy2)(const uint8_t *ysrc, const uint8_t *usrc, const uint8_t *vsrc, uint8_t *dst, 2-long width, long height, 3-long lumStride, long chromStride, long dstStride, long vertLumPerChroma)进行了标记修改。 有趣的是，这个修补程序更改了5个文件，超过6K的更改，所有更改都是格式化更改。 这次提交中的大多数函数都被ActiveClean建议进行标签更正。

内部有效性威胁：我们面临的一个重要挑战是需要对已清理的提交进行真实性验证。因此，我们为我们的Java项目使用了由[19]提供的数据集，该数据集要求四位不同的作者审查标签，其中至少有3位作者同意标签。我们为我们的C项目生成了两位作者一致性协议的真实性验证，遵循了[21]的方法。为确保不存在模型参数选择偏见，我们在模型选择中使用了完全不同的项目，而不是在实验中使用的项目。我们还在实验中使用了3折交叉验证，以确保我们的结果不显示选择偏见。

我们还对没有地面真实数据的结果进行了抽样，并由两位作者进行了手动验证。

为了减轻外部威胁，我们使用了两个数据集中的地面实况标签，这些数据集包含超过4.3K次提交和119K次提交行的Java和C语言实际开源项目。这些项目在目的、规模和作者方面都非常不同，以确保ActiveClean不会学习并报告特定于项目的特征。我们在17个Java项目和2个C语言实际存储库上评估了ActiveClean，这些存储库来自广泛使用的漏洞数据集Devign。据我们所知，LineVul、LineVD和IVDetect是三种基于行级的漏洞检测工具。只有LineVul能够成功运行，并且它是领先的技术。

相关工作
多项研究强调了在漏洞检测任务中需要大量高质量数据的必要性。Croft等人[11]研究了软件漏洞数据集中数据质量的重要性，揭示了漏洞标签不准确和重复数据点可能导致模型训练无效和结果不可靠的问题。Wu等人[44]讨论了数据集标签不准确对预测决策的不利影响，Chakraborty等人[7]确定了重复数据、不足的基于标记的模型、无关的特征学习以及数据不平衡导致现有模型在应用于现实问题时性能不佳。Herbold等人[19]开展了一项手动验证大量bug修复提交的研究，突显了手动验证的局限性以及与数据质量有关的与bug无关的行对结果的影响。

现有的与漏洞相关的数据集的粒度不一，从文件级到功能级不等。其中，Reveal使用公开可用的补丁，Devign使用经关键字筛选的提交中手动标记的数据，而Big-Vul则同时利用公共漏洞和 CVE 数据库描述以及提交来生成功能级漏洞数据集。余等人建议利用用户反馈使用主动学习来改进文件级漏洞预测。

与我们的工作相比，这些作品缺乏训练一款好的逐行漏洞检测工具所需的细致度。在逐行粒度上，BugBuilder [23]和D2A [51]使用测试用例和差分静态分析来排除不相关的更改。然而，由于其过滤方法的限制，这两种方法在可以处理的漏洞类型方面受到限制。相比之下，我们的方法通过提取的特征中的模式来学习区分与漏洞相关和与漏洞无关的行，并且不会受到这些限制的影响。

过去，有许多关于处理错综复杂提交的研究。这些研究利用测试[18,48]、图聚类[8,29,39]或基于反馈的工具[26,42]来识别错综复杂的更改。DEPTEST [48]和Delta Debugging [18]利用自动化测试来过滤混乱的代码更改。Kirinuki [26]和CoRA [42]创建了工具，用于警告或帮助用户避免首次进行错综复杂的更改。ComUnt [8]、SmartCommit [39]、UTango [29]和Flexeme [36]采用各种基于图的方法来解决这个问题，包括图分区。

结论和未来工作
本文介绍了ActiveClean，一个用于生成基于行级漏洞数据的自动化且可扩展的工具。ActiveClean利用主动学习显著减少了训练模型所需的标记数据量。我们设计了考虑到提交行及其周围代码的特征。在我们的评估中，我们使用了Java和C数据集，处理了超过4.3K个提交和119K行提交。我们发现提交中存在很多噪音，并且许多提交可能含有这种噪音。ActiveClean能够找到不相关于漏洞的行，并报告了70-74之间的F1分数。使用ActiveClean，我们为Devign数据集中的FFmpeg生成了行级漏洞数据。我们展示了使用这个经过清洗的数据集，LineVul能够在87%的F1分数下检测到更多的易受攻击函数，以及在73%的前10中检测到更多的易受攻击行数，相较于基线报告的83 F1分数和66%的前10。通过对ActiveClean推荐的50个标签进行纠正，我们发现了FFmpeg的29个不正确的函数级标签。在未来，我们将继续使用ActiveClean生成更多的行级数据集，并报告当前漏洞数据集中的错误标签。ActiveClean: 通过主动学习生成行级漏洞数据 ArXiv，2023年12月，美国埃姆斯。
这篇论文是一项有关大型语言模型在自动程序修复领域的系统文献综述。来自中国南京大学新软件技术国家重点实验室的张全军、方春荣、谢阳和马宇翔，以及新加坡南洋理工大学计算机科学与工程学院的孙伟松、澳大利亚斯威本科技大学计算技术系的杨云，以及再次来自南京大学新软件技术国家重点实验室的陈振宇。自动程序修复（APR）的目标是修补软件缺陷，减少手动调试工作量。最近，随着大型语言模型（LLMs）的进展，越来越多的APR技术被提出，显著促进了软件开发和维护，并展现出卓越的性能表现。然而，由于在基于LLM的APR领域持续进行探索，研究人员难以了解当前的成就、挑战和潜在机会。本文提供了第一部系统文献综述，以总结2020年至2024年之间LLMs在APR中的应用。我们分析了来自LLMs、APR及它们整合视角的127篇相关论文。首先，我们将应用于支持APR的现有热门LLMs进行分类，并概述了三种部署策略。此外，我们详细介绍了一些受益于LLMs的特定修复场景，例如语义缺陷和安全漏洞。此外，我们讨论了将LLMs整合到APR研究中的一些关键方面，例如输入形式和开放科学。最后，我们强调了一系列待调查的挑战以及未来研究的潜在指导原则。总的来说，我们的论文为APR社区提供了研究领域的系统概述，帮助研究人员全面了解成就并推动未来研究。我们的研究数据公开托管在GitHub上：https://github.com/iSEngLab/AwesomeLLM4APR。

CCS概念：•软件及其工程→软件测试和调试。

关键词和短语: 大型语言模型, 自动化程序修复, LLM4APR 1 引言 软件缺陷被认为是不可避免且破坏性的，给全球用户带来安全隐患，每年造成数十亿美元的财务损失[11,156]。开发人员手动修复检测到的软件bug非常复杂且耗时[13]。自动化程序修复（APR）在软件开发和维护中扮演着至关重要的角色，旨在修复软件bug而无需人工干预。继2009年的基础工作GenProg[80,155]之后，APR在过去几十年中得到了广泛研究[43,105]，研究人员提出了各种APR技术，包括基于启发式的[64,80,98,177]、基于约束的[31,99,169,171]和基于模式的[76,91,92]。最近，受深度学习（DL）的进展启发，越来越多基于学习的APR技术被提出，利用神经网络模型自动学习修复bug的模式[18,66,84,85,96,142,174,175,199,200]。由于DL模型学习来自大量代码语料库中的隐藏修复模式的强大能力，学习型APR在过去几年取得了显著表现[182]，引起了学术界和行业的广泛关注[69,70,73]。

作者地址：Quanjun Zhang, quanjun.zhang@smail.nju.edu.cn；Chunrong Fang, fangchunrong@nju.edu.cn；Yang Xie, serialxy@outlook.com；Yuxiang Ma, 502022320009@smail.nju.edu.cn，南京大学新软件技术国家重点实验室，中国南京，210093；Weisong Sun, weisong.sun@ntu.edu.sg，新加坡南洋理工大学计算机科学与工程学院，50 Nanyang Avenue，新加坡，639798；Yun Yang, yyang@swin.edu.au，澳大利亚墨尔本斯文本科技大学计算技术系，墨尔本，3122；Zhenyu Chen, zychen@nju.edu.cn，南京大学新软件技术国家重点实验室，中国南京，210093。 最近，大型语言模型（LLMs）已成功应用于广泛的与源代码相关的任务[147,184]，如代码生成[82,148,150,201]，代码摘要[132,133,146]和测试生成[4,24,57,108,128]。受益于庞大的模型参数和海量训练数据，LLMs展现出令人印象深刻的性能，并从根本上改变了软件工程领域的研究范式。在自动程序修复方面，从开创性研究开始，例如TFix [7]，CIRCLE [176]和AlphaRepair [163]，该领域目睹了利用LLMs进行修复研究激增的情况，已经取得了相当大的优势，并进一步显示了未来研究的重要潜力。然而，LLMs与自动程序修复的整合是一个极其复杂的任务，这使得有兴趣的研究人员很难理解现有工作。例如，现有基于LLMs的自动程序修复研究涵盖了不同的研究视角（如实证[162]，技术[163]和基准研究[187]），修复阶段（如补丁生成[186]和正确性评估[183]），修复场景（如静态警告[69]和语法错误[70]），模式架构（如仅编码器[185]和仅解码器[100]）以及模型利用范式（如微调[176]，少样本学习[108]和零样本学习[186]）。尽管该领域不断探索，但目前文献缺乏对LLMs在自动程序修复中应用的详细系统审查，这使得研究人员难以理解现有工作的多样化设计选择，并进行后续研究。

这篇论文旨在弥合现有快速发展的LLM技术在APR研究中应用的空白。我们的工作提供了关于LLM技术在APR研究中部署的首个系统文献综述。通过这项工作，研究社区可以全面了解现有LLM技术在APR中的优势、劣势和存在的空白。我们讨论了当前在最前沿的APR研究中广泛采用的LLM技术以及它们如何融入修复工作流程。我们收集了127篇相关论文，并从LLM技术、APR和融合的角度进行系统分析。通过我们的分析，揭示了当前的挑战，并指出了LLM技术在APR研究中可能的未来方向。总体而言，这项工作全面概述了LLM技术在APR社区中正在进行的进展，帮助研究人员在这一新兴领域中导航，并朝着创新实践迈进。

贡献。总的来说，这项工作有以下几点贡献：
• 调研方法。我们进行了第一次系统文献综述，研究了2020年至2024年4月使用最新LLMs来解决修复挑战的127篇高质量APR论文。

•趋势分析。我们对选定的APR研究进行了详细分析，主要针对出版趋势、出版期刊分布以及研究贡献类型进行研究。

我们总结了46种用于支持程序修复的LLM（机器学习模型），并提供了在程序修复领域中不同LLM类别的典型用途和趋势摘要。

在APR的观点下，我们描述了常见的修复方案，这些方案被应用于LLMs，囊括了18种Bug类型，比如安全漏洞和编程问题。

• 整合视角。我们讨论了一些关键因素，包括数据集、输入表示和开放科学，这些因素影响了将LLMs整合到自动程序复审中的性能。

挑战与机遇。我们总结了在自动程序理解（APR）领域应用LLMs（大型语言模型）所面临的一些关键挑战，并指出了未来基于LLMs的APR研究的一些潜在指导方针。

文献组织。第2部分介绍了有关APR和LLMs的一些基本概念。然后，根据上述贡献，第3部分列出了我们的研究问题（RQs）和用于收集与我们工作相关的论文的研究方法。第4部分研究了基于LLM的APR研究的趋势和分布。第5部分总结了现有APR研究中使用的LLMs。第6部分说明了LLMs应用的主要修复场景，并对每项工作进行了简要描述。第7部分讨论了LLMs和APR集成过程中的一些关键因素，包括数据集、输入表示、补丁正确性和开放科学。

第8部分讨论了一些挑战和实用指南。第9部分总结了。

## 2 背景和相关工作 
### 2.1 自动化程序修复 
#### 2.1.1 问题描述 
自动程序修复（APR）是软件工程领域中最具挑战性的活动之一，旨在在不需要人工干预的情况下修复软件缺陷，并被视为软件自动化的一个基本方面。例如，在软件开发和维护过程中，开发者通常尝试根据需求规格编写测试套件以验证所设计功能的正确性。如果所有测试用例都通过，即认为功能被正确实现；否则，开发者必须分析失败的症状并对有缺陷的代码片段进行必要的修改，以确保所有测试用例都通过。在形式上，给定一个有缺陷的程序 𝑃 和其相应的规格 𝑆，以致于 𝑃 无法满足，APR 的定义是找到一个最小的转换 𝑃′，使得它满足规格 𝑆。在实践中，通常将测试用例用作规格，而在典型的测试驱动修复情景下，APR 的目标是找到一个最小的程序变体，通过现有的测试套件，其中至少包含一个使原有有缺陷程序 𝑃 无法满足的测试用例。

自动化程序修复的基本流程如图1所示，包括测试套件的生成、故障定位、补丁生成、补丁验证、以及正确程序候选补丁的生成和怀疑元素。

2.1.2 修复工作流程。图1展示了典型的自动生成和验证的APR工作流程，通常由三部分组成。具体来说，对于一个检测到的错误，(1) 故障定位阶段通过现成的定位技术 [158] 找出需要修复的可疑代码元素；(2) 补丁生成阶段通过转换规则 [182] 生成程序变体（即候选补丁）；(3) 补丁验证阶段利用可用的测试套件作为参照，通过动态执行 [183] 识别正确的补丁。需要注意的是，一个成功通过测试套件的候选补丁被称为合理的补丁。

然而，如果一个看似可行的补丁无法推广到额外的测试案例，那就被认为是一个过度拟合的补丁。相反，一个在更广泛测试案例中仍然有效的可行补丁被认定为一个正确的补丁，一个在语义上与开发人员手动编写的补丁一致的补丁。

修复技术。文献中介绍了许多自动程序修复（APR）技术，从不同角度生成正确的补丁。这些APR技术可以分为四类：（1）基于启发式的APR利用遗传编程来探索正确补丁的搜索空间，例如GenProg [80]，Astor [98]，ARJA [177]和SimFix [64]；（2）基于约束的APR通常专注于条件综合问题，将程序修复视为约束求解任务，例如Nopol [171]，Cardumen [99]，ACS [169]和Dynamoth [312]；（3）基于模式的APR利用预定义的修复模板，通常由专家手工制作，将有缺陷的代码片段转换为正确的代码，例如TBar [92]，FixMiner [76]和Avatar [91]；（4）基于学习的APR将补丁生成视为一项神经机器翻译任务，借助DL模型的进展，例如Tufano等人[142]，SequenceR [18]，CoCoNut [96]，DLFix [84]，CURE [66]，Recoder [199]，DEAR [85]，SelfAPR [174]，RewardAPR [175]和Tare [200]。

在四种APR技术中，基于学习的APR通过从大量的源代码数据库中自动学习隐藏的bug修复模式，取得了显著的性能。

最近受到LLMs在自然语言处理和软件工程任务中的成功启发，研究人员越来越多地开始利用先进的LLMs来解决软件缺陷。与基于学习的自动程序修复相比，这种基于LLMs的自动程序修复技术表现出明显更好的性能，并在社区中受到越来越多的关注，这也是我们工作的重点。

2.2 大型语言模型

2.2.1 初步介绍。大型语言模型指的是经过大规模文本语料预训练的先进人工智能模型，以增强其类人自然语言理解、生成和解释能力[193]。通常情况下，大型语言模型具有巨大数量的参数，远远超过传统深度学习模型的规模，因此能够在各种任务中发挥作用，比如问题回答和机器翻译。

在1986年，Rumelhart等人引入了循环神经网络（RNNs），通过在内部状态中保持先前输入的记忆，开启了处理序列数据的可能性。1997年，Hochreiter等人引入了RNNs的扩展，即长短期记忆网络（LSTMs），通过一个单元状态和三种类型的门来解决长期依赖问题。2017年，Vaswani等人引入了Transformer，通过自注意力机制对输入数据的不同部分进行重要性加权，奠定了LLMs的基础。从RNNs到LSTMs再到Transformers的演变代表了更有效地处理更长序列和更复杂数据模式的趋势，尤其是对涉及自然语言处理的任务。特别是，Transformers以其更有效地处理长程依赖关系的能力和其适用于并行计算的特点，在许多人工智能研究和应用领域中已成为主导的模型架构。

LLMs（大型语言模型）通常是基于上面提到的Transformer架构构建的，采用预训练和微调的范式。具体来说，这些模型经过预训练，通过在大量未标记的数据上进行自监督学习获得通用语言表示。随后，它们通过对有限数量的标记数据进行监督微调，以适应各种下游任务。由于先进的模型架构和训练范式，LLMs已经在各种自然语言处理领域取得了突破，尤其是像BERT、GPT、T5、PaLM、LLaMA、LLaMA2 、LLaMA3等模型。最近，在源代码领域的研究工作迅速增长，出现了一些重要进展，如CodeBERT、CodeGPT、CodeT5、InCoder、CodeGen、CodeLlama、StarCoder和StarCoder2等模型。

2.2.2 模型类别。文献中涉及支持自然语言处理（NLP）和软件工程（SE）研究的各种大型语言模型（LLM），可以根据它们的模型架构分为三大类别。 (1) 仅编码器的LLM，如CodeBERT [35]、GraphCodeBERT [46]，训练Transformer的编码器部分，通过掩码语言建模（MLM）和下一句预测（NSP）生成一个固定维度的双向表示。MLM旨在预测被随机掩码的原始标记，而NSP则预测两个给定的句子是否实际上在文本中相互跟随。 (2) 仅解码器的LLM，如CodeGPT [95]，训练Transformer的解码器部分，支持具有因果语言建模（CLM）的自回归任务，该任务旨在基于先前标记预测序列中的新标记。 (3) 编码器-解码器LLM，如CodeT5 [152]，训练Transformer的编码器和解码器部分，支持具有去噪目标的序列到序列生成任务。我们将在第5.1节总结现有的LLM及其如何被利用来支持程序修复。

2.3 相关研究 在这项调查中，我们系统地收集、总结和分析了利用先进LLM技术赋能的自动程序修复（APR）研究。尽管我们的主要重点是LLM与APR的整合，鉴于其日益突出，还存在一些相关研究探讨了LLM或APR的类似方面。

我们在下文中澄清我们的工作与先前工作之间的基本差异。

第一组相关调查试图更广泛地分析在软件工程中对LLM的应用。 张等人 [184] 从两个直观的角度对基于LLM的SE研究进行了详细总结：LLMs及其在SE中的应用。 他们介绍了30个LLMs、15个预训练任务、16个下游任务以及43个特定的与代码相关的任务的代表性工作。

另外，王等人提供了有关在软件测试中的LLMs的综述；樊等人讨论了软件工程中LLMs的成就和挑战；侯等人进行了一项关于LLM4SE的系统文献综述。然而，这些研究的重点是整个软件工程/测试工作流程，而不是我们特定的程序修复任务。它们只提供了有限数量基于LLM的自动程序修复论文的概览，而我们的工作则提供了从各种视角深入分析。

他们的研究表明，在软件工程中，程序修复是最常见的场景，LLMs 在这个领域被应用得很广泛。鉴于程序修复的复杂性和重要性，我们被激发去进行一次更深入的研究，审视涉及 LLMs 的 APR 成就。

第二组相关调查试图回顾自动程序修复（APR）的成就。Gazzola等人[43]提出了一项整理修复研究的调查，Monperrus等人[105]提出了一个关于行为和状态修复研究的文献目录。这两项工作收集了截至2017年的出版物，因此与我们的研究没有重叠，因为第一个基于LLM的APR工作是在2020年发表的。与本文最相关的工作是张等人[182]提供的系统调查，总结了学习型APR社区的最新技术。然而，他们主要关注DL模型，仅在讨论基于LLM的APR研究方面给予一个小节（仅包含五篇论文），未能涵盖过去两年涌现的大量工作。相反，我们的工作重点放在LLM在APR社区中的应用技术上，涵盖了直到2024年4月的使用趋势，优化技术，修复场景和输入形式。

3. 调查方法学
在这一部分，我们根据Petersen等人[117]和Kitchenham等人[74]提出的原则，介绍我们系统文献综述方法的详细内容。

3.1 研究问题 我们试图通过总结相关研究并进一步提供关于后续研究的指导，来全面概述最近LLMs在APR中的应用。为了实现这一目标，这篇系统文献综述回答以下研究问题（RQs）： •RQ1：利用LLMs的APR研究趋势是什么？ •RQ2：哪些流行的LLMs被应用来支持APR？ •RQ3：LLMs已经促进了哪些维修场景？ •RQ4：哪些关键因素有助于LLMs在APR中的整合？ 张全军，方春荣，谢阳，马宇翔，孙伟松，杨云，陈振宇 3.2 搜索策略 我们使用“准黄金标准”（QGS）[178]方法进行搜索和识别主要论文，这是一种常见的做法，通过将手动和自动搜索过程相结合来构建一组已知研究，以便通过细化搜索字符串。 我们首先进行手动搜索以识别一组相关研究，并从中得出搜索字符串，详见第3.3节。然后，我们利用搜索字符串进行自动搜索，并采用一系列相对严格的过滤步骤获取最相关的研究，详见第3.4节。最后，我们利用滚雪球搜索来进一步补充搜索结果，详见第3.5节。鉴于来自不同研究领域（如SE和AI）的相关论文众多，这种搜索策略使我们能够捕捉到最相关的论文，同时比纯手动搜索更加高效，具有相对严格的过程。特别是，我们进行以下五个阶段来搜索和识别相关研究。

(1)搜索来源。这一步骤旨在为首次手动搜索选择高质量的出版物发布渠道，并为随后的自动化搜索选择知名的数字数据库。

（2）QGS的建立。这个阶段检查了手动搜索中找到的所有文件，并按照包含/排除标准对其进行过滤。

（3）搜索项目。在这个阶段，通过领域知识和建立的QGS 来定义搜索项目。

(4) 文献收集和选择。在这阶段，通过使用上述搜索项进行自动搜索，并按照纳入/排除标准筛选收集到的论文。

(5) 知识积累搜索阶段。 这个阶段进行知识积累搜索，以补充最终的搜索结果。

3.3 检索项目
我们选择了四个顶级软件工程会议（ICSE、ESEC/FSE、ASE、ISSTA）和两个期刊（TOSEM和TSE），进行手动搜索并建立了QGS，搜索涉及程序修复和LLM的相关论文。我们手动检查并确定了符合我们标准的2020年至2024年的25篇论文，这些论文将成为构建QGS的基础。根据之前的调查 [149, 172, 182]，我们将用于自动搜索论文的搜索项目分为两组：（1）包含与程序修复相关的一些常用关键词的APR相关组；（2）包含一些与大型语言模型或预训练模型相关的热门关键词的LLM相关组。此外，由于APR社区受益于维护良好的论坛和实时更新的工作，我们通过总结三个来源的一些常见关键字进一步完善了我们的搜索项目，这三个来源是：社区驱动的网站1、Monperrus的APR活生生的综述 [106] 和最新的基于学习的APR调查 [182]。最后，我们确定了一个搜索字符串，其中包含在使用LLM进行程序修复研究中频繁出现的与LLM和APR相关的关键词。完整的搜索关键词集如下：（“program repair” OR “software repair” OR “automatic repair” OR “code repair” OR “bug repair” OR “bug fix” OR “code fix” OR “automatic fix” OR “patch generation” OR “patch correctness” OR “patch validation” OR “fix generation” OR “code transformation” OR “code edit” OR “fix error”）AND（“LLM(s)” OR “Large Language Model(s)” OR “Pre-trained” OR “Pretrained” OR “Pre-training” OR “Pretraining” OR “PLM(s)” OR “(Code)BERT” OR “(Code)T5” OR “(Code)GPT” OR “Codex” OR “ChatGPT” OR “(Code)Llama” OR “GPT-*” OR “neural” OR “machine” OR “deep” OR “learning” OR “transformer/transformers” OR “model/models” OR “transfer” OR “supervised”）。
1 http://program-repair.org/bibliography.html 关于用于自动程序修复的大型语言模型的系统文献综述 1:7 表1。包含标准和排除标准。

纳入标准：① 文章在其框架中使用了LLM。

这篇论文讨论的是程序修复的任务。

3. 这篇论文的全文可供阅读。

这篇论文只有英文版本。

排除标准 ❶ 这篇论文少于七页。

这篇论文是由同一组作者将一份老版本的会议论文扩展转化为期刊论文。

这篇论文使用修复方法来为LLMs做出贡献。

这篇论文只是作为一个系统性文献综述、评论或调查, 仅仅提及了LLMs和APR的发展。

这篇论文发表在研讨会或博士研讨会上。

这篇论文是一种灰色出版物，比如技术报告或论文。

这篇论文属于短篇论文、工具演示和社论类别。

表2. 基于LMM的APR研究质量评估标准（QAC）检查表。

ID质量评估标准：QAC1 是否以LLM作为研究对象，而不仅仅作为基准方法？QAC2 文章对APR社区的影响是否清晰陈述？QAC3 文章的贡献是否清晰陈述？QAC4 文章是否发表在声誉高的期刊或会议？QAC5 相关工件，例如数据集，是否已开源？QAC6 文章是否有明确的动机？QAC7 是否清晰描述了所提出方法的实施过程？QAC8 实验设置是否详细描述，例如超参数和环境？QAC9 实验结果是否清晰支持研究发现？QAC10 是否讨论了研究的主要贡献和局限性？值得注意的是，我们还包括了与张等人所提供的与机器/深度学习相关的关键词列表，以尽量避免错过与我们工作相关的任何论文。

3.4 研究文献的收集与筛选
为了进行自动化搜索，我们主要在2024年4月底搜索Google Scholar数据库、ACM数字图书馆和IEEE探索器数字图书馆，收集潜在相关的论文。一旦我们根据自动搜索策略收集到研究文献，我们将进行过滤和去重阶段，排除与研究目标不符的论文。首先，我们的目标是筛选出2017年之前发表的论文，考虑到Transformer架构[143]是在2017年引入的，这个架构为LLM奠定了基础。其次，我们去除重复的论文，因为同一篇论文可能被多个数据库索引。第三，我们排除少于七页的论文[182]（排除标准1），最终得到283篇论文。

最后，我们通过手动检查论文的标题、摘要、关键词和发表地点，根据表1中的纳入/排除标准包含相关的论文，将论文数量减少到241篇。 值得注意的是，在排除标准❺方面，我们考虑了国际自动化程序修复研讨会，考虑到其与自动程序修复领域的相关性和高质量发表。 最后，我们对剩余的论文进行全文检查，以确定它们是否与基于LLM的自动程序修复领域相关且质量高。 我们按照Wang等人[149]的做法，回答十个问题来评估包含论文的相关性和质量，这些问题列在表2中。 关于QAC4问题，考虑到这一领域的新兴性以及许多作品，1:8 Quanjun Zhang, Chunrong Fang, Yang Xie, Yuxiang Ma, Weisong Sun, Yun Yang和Zhenyu Chen，15146542 010203040506070 20202021202220232024（a）每年的出版物数量。

每年发表的累积论文数量为：162085127, 020406080100120140, 20202021202220232024。

图2：基于LLM的APR研究的出版趋势。

尤其是那些涉及最近发布的LLM的论文，还没有完成同行评审过程。我们考虑来自arXiv的论文，并通过质量评估过程选择高质量的论文，以使我们的调查更全面和及时。我们获得了与我们的工作相关的110篇论文。

我们还采用了雪球搜索的方式，以确保我们收集到的论文尽可能全面。雪球搜索是一种常见的做法，通过手动检索可能在先前搜索过程中被忽视的任何潜在论文。雪球搜索包括查阅所收集论文中的参考文献（即向后雪球搜索）或引用文献（即向前雪球搜索），以发现更多相关的作品。具体来说，我们会仔细审查所收集论文中的每个参考文献和引用文献，以评估它们与我们研究目标的相关性。通过整个研究选择过程中对所有潜在论文的细致手动分析，最终导致我们调查报告中包含了127篇论文。

4 RQ1：利用LLMS的APR研究趋势是什么？4.1 随着时间的推移，出版趋势是什么？我们分析了在2020年至2024年4月期间利用LLMs增强的APR研究的出版趋势。尽管Transformer架构[143]在2017年提出，开创性的LLM，BERT[23]在2018年推出，但我们并未发现在2020年之前有任何使用LLMs修复错误的研究。图2(a)展示了相关研究的数量，我们发现从2020年到2023年的出版数量呈显著增长趋势，2023年的论文数量达到了65篇。值得注意的是，我们截至2024年4月收集论文；因此2024年的相关研究数量（即42篇论文）不能反映2024年LLM为基础的APR的总体趋势。我们将出版数量作为幂函数拟合，以预测过去四年的出版趋势，并发现2020年至2023年，拟合分布的斜率显著增加。特别是，决定系数达到最高值(𝑅2=0.973)，估计2024年可能有超过90篇使用各种LLMs解决错误修复的相关论文。我们还计算了如图2(b)所示的累积出版量。我们进行同样的幂函数拟合(𝑅2=0.985)，估计到2024年底将有超过140篇论文，这表明利用LLMs解决APR挑战的相关研究数量未来有望大幅增长。总体而言，自2020年以来，利用LLMs解决错误修复问题已成为一种普遍趋势，未来将有大量研究采用LLMs来解决APR的挑战。

表格 3. 发表低亮度模式（LLM）为基础的自动程序复审（APR）研究的出版场所。

缩写 Venues Papers Proportion arXiv N.A. 55 43.31% ICSE 国际软件工程会议 13 10.24% ESEC/FSE 欧洲软件工程会议和国际软件工程基础研讨会 9 7.09% APR 国际自动程序修复研讨会 6 4.72% ICLR 国际学习表征会议 5 3.94% ASE 国际自动化软件工程会议 6 4.72% TOSEM 软件工程方法论期刊 4 3.15% TSE 软件工程期刊 3 2.36% ISSTA 软件测试与分析国际研讨会 2 1.57% AAAI 人工智能国际会议 1 0.79% ACL 计算语言学协会 1 0.79% COMPSAC 国际计算机软件与应用会议 1 0.79% GECCO 遗传与进化计算会议 1 0.79% ICSME 软件维护与演化国际会议 1 0.79% MSR 软件库挖掘国际会议 1 0.79% NeurIPS 神经信息处理系统会议 1 0.79% OOPSLA 面向对象编程系统、语言及应用会议 1 0.79% PLDI ACM SIGPLAN 编程语言设计与实现会议 1 0.79% PMLR 机器学习研究论文集 1 0.79% S&P IEEE 安全与隐私研讨会 1 0.79% SIGCSE ACM 计算机科学教育特别兴趣小组 1 0.79% SLE ACM SIGPLAN 软件语言工程国际会议 1 0.79% TDSC IEEE 可靠与安全计算期刊 1 0.79% TIFS IEEE 信息取证与安全期刊 1 0.79% WWW ACM 网络会议论文集 1 0.79% Others N.A. 8 6.30% Total N.A. 127 100.00% 

4.2 出版地点的分布是怎样的？我们分析了127篇发表在各种出版地点的研究。表3列出了每个出版地点上发表的相关论文数量。我们发现56.69%的论文发表在同行评议的地点，其中ICSE和ESEC/FSE是含有13篇论文最多的热门地点，其次是APR（六篇论文）、ICLR（五篇论文）和ASE（四篇论文）。此外，热门的地点大多是顶级会议和研讨会，只有四种期刊至少有一篇相关论文，即TOSEM（四篇论文）、TSE（三篇论文）、TDSC（一篇论文）和TIFS（一篇论文）。这一发现表明出于会议文章的及时性，人们更倾向于在会议上展示最新的工作。这些收集的论文发表在不同的研究领域中，包括软件工程（如TOSEM和ICSE）、人工智能（如NeurIPS、AAAI和ICLR）、自然语言处理（如ACL）、编程语言（如OOPSLA和PLDI）以及安全领域（如S&P、TDSC和TIFS）。

我们还发现，其余43.31%的论文未经同行评议，是在arXiv上发表的。

这种现象可能是由于相关研究在短时间内迅速涌现。考虑到这些非同行评审的论文质量参差不齐，我们进行了严格评估，以确保将高质量的相关论文纳入本研究。

4.3 编程语言的分布是怎样的？图3展示了现有基于LLM的自动程序修复技术所针对的不同编程语言的分布情况。我们发现，Java是最广泛使用的编程语言，占所有收集到的论文的37%，其次是Python（24%）和C（11%）。可能的原因在于这些语言有成熟的数据集可供使用，这些数据集被用作评估修复技术的公认基准，例如Defects4J，QuixBugs，BFP，CVEfixes等。分布图3展示了基于LLM的研究在不同编程语言中的分布情况。

我们发现，基于LLM的自动程序修复（APR）涵盖的编程语言范围比传统的APR更广。例如，我们收集的论文涉及总共18种不同的编程语言，而基于学习的APR技术通常仅限于五种语言。更重要的是，我们注意到一些之前在APR社区中被忽视的罕见语言现在正在被关注。基于LLM的APR具有更广泛的语言适应性，这可能源于LLM本身具有封装通用编程知识的能力，可以通过微调在多种语言之间进行传递。

此外，大型语言模型具有强大的自然语言理解能力，可以在学习样本有限的情况下进行几次或零次学习样本的修复设置，这是与通常需要大量修复语料库进行训练的深度学习模型相比的一个重要优势。因此，大型语言模型可以高效处理像Verilog和Rust这样广为人知的编程语言，这些编程语言在以前的自动程序修复研究中通常未被充分研究。这种现象突显了近期大型语言模型带来的自动程序修复的前景和可扩展性。

4.4 出版物贡献的类型有哪些？请参考表格 4。这些类型主要分为四大类。

该研究提出了一种新颖的方法来解决由LLMs技术赋予的软件自动程序修复（APR）领域特定挑战。实证研究探讨了LLMs在定量和定性分析中修复软件缺陷的性能。这篇论文引入了一个专门设计用来评估LLMs在不同缺陷类型下修复能力的新基准。人类研究通过调查探索从业者在使用LLMs修复软件缺陷时的观点和经验。根据主要贡献，我们将收集的论文分为四类：新技术或新方法、实证研究、基准测试和人类研究，如图4所示。

我们发现已经发表了78篇相关论文，旨在提出一种新的修复方法或框架，使用LLMs来解决在自动程序修复社区中的各种问题。此外，还有38篇论文集中于进行实证研究，探讨LLMs在修复各种问题中的实际好处。图4显示了自动程序修复中使用的现有LLMs的分布情况。

图5显示了利用大型语言模型进行自动程序审阅的三种方式的分布情况：微调37%、少样本学习15%、零样本学习48%。

研究人员发现了一些问题，比如对大语言模型进行微调以修复漏洞的潜力。此外，我们还注意到有九项相关研究正在构建新的基准来评估大语言模型的性能，以及有两篇论文对从业者或开发者如何在实践中运用大语言模型来修复软件缺陷进行调查，以提供更多见解。

RQ1研究结果摘要：(1) 在2020年至2024年间，LLM在修复软件缺陷方面呈现出蓬勃发展的趋势，共有127篇论文。 (2) 使用LLM进行自动程序修复的会议论文数量显着超过期刊论文数量，其中ICSE和TOSEM分别是最受欢迎的会议和期刊。 (3) 基于LLM的自动程序修复论文发表在不同的研究领域，包括软件工程、人工智能和安全领域。 (4) LLM在18种编程语言上都有应用，其中Java、Python、C和C++是应用最频繁的语言。 (5) LLM已被应用于一些较少被关注的编程语言，如Verilog和Rust。 (6) 绝大多数收集的研究主要集中在介绍新技术和进行实证研究，而有两篇论文进行了用户研究，以了解从业者在利用各种LLM解决软件缺陷任务时的态度和经验。

问题2：已经应用于支持自动程序修复的流行大型语言模型有哪些？分析涉及的大型语言模型分布，其中有数百万到数十亿（甚至更多）参数的大量大型语言模型被提出和调整用于自动修复软件缺陷。我们分析了所有收集到的论文，并总结了46个相关的大型语言模型，这些模型已经在现有的自动程序修复研究中被使用。图4展示了不同大型语言模型的多样性，并列出了这些模型在先前的自动程序修复研究中被使用的次数。

由于页面限制，我们只包含那些在收集的论文中出现超过一次的LLM。如图4所示，来自OpenAI的ChatGPT（37）和GPT-4（25）是最受欢迎的两个LLM，接着是CodeT5（23）和Codex（21）。我们根据它们的架构将这些LLM分为三组：仅编码器、编码器-解码器和仅解码器LLM。

编码器-仅语言模型 (Encoder-only LLMs) 指的是一类只使用 Transformer 架构的编码器堆栈的语言模型。通常，这些模型通过使用掩码语言建模（MLM）任务在大规模数据集上进行预训练，从上下文中学习预测掩码词的身份。在软件缺陷自动修复领域，像 CodeBERT 和 GraphCodeBERT 这样的知名模型已经在语义错误和安全漏洞方面得到了研究。由于这些语言模型只包含一个编码器组件，能够为输入生成上下文感知的表示，因此它们特别适用于代码理解任务，例如代码搜索，但并非直接适用于代码生成任务，如程序修复。因此，在软件缺陷自动修复领域，正如张等人所述，研究人员可能需要集成一个新的解码器，从头开始初始化预训练的编码器，以构建一个用于修补生成的编码器-解码器架构。我们还注意到这种仅包含编码器的语言模型被用于确定修补程序的正确性，详情请见第7.3节讨论。

5.1.2 编码器-解码器LLMs。编码器-解码器LLMs是指一类LLMs，它们利用Transformer架构中的编码器和解码器堆栈，因此非常适合将一个序列转换为另一个序列。具体而言，编码器接受一个序列作为输入，并将其编码为一个固定大小的隐藏状态，这有效地捕捉了输入序列的语义和含义。然后解码器处理隐藏状态，并使用注意力机制生成相应的输出序列，必要时参考输入序列的部分。由于编码器-解码器架构，这种LLMs特别适用于序列到序列学习环境中的代码生成任务，比如程序修复。在APR领域，大量的编码器-解码器LLMs被广泛采用，如CodeT5（23）[152]，PLBART（10）[2]，UniXcoder（4）[45]和T5（3）[121]。与传统的基于学习的APR类似，这些研究通常将APR视为一种自然的机器翻译（NMT）任务，通过监督的序列到序列学习来进行，例如CIRCLE [176]，TFix [7]，VulRepair [39]和RAP-Gen [151]。

5.1.3 解码器专用的语言模型(Decoder-only LLMs)。解码器专用的语言模型是指只使用Transformer架构中解码器堆栈的一类语言模型。解码器专用的语言模型通常通过因果语言建模（CLM）目标进行预训练，学习根据前文预测句子中的下一个词。这些模型专门设计用于自回归地生成文本序列，即一次生成一个标记，利用迄今为止生成的内容作为后续标记的上下文。在APR领域中，与仅编码器和编码器-解码器语言模型相比，解码器专用的语言模型是最受欢迎和广泛使用的群体。解码器专用的语言模型的显著修复应用包括GPT系列模型（例如GPT-1 [119]、GPT-2 [120]、GPT-3 [14]、GPT-3.5 [111]、ChatGPT [112]和GPT-4 [113]），一些开源模型（例如CodeGPT [95]、GPT-Neo [10]、GPT-NeoX [9]、GPT-J [145]、InCoder [37]、CodeGen [109]、CodeLLaMA [124]和StarCoder [83]），以及一些闭源模型（例如Fan等人的Codex [16]和CEDAR [108]）。解码器专用的语言模型基于APR研究的兴起主要有两个原因。第一个原因是，这些模型可以自然地从少量示例或简单指令中进行程序修复，而无需任何微调。第二个原因是，随着主要互联网公司推出商业产品，如OpenAI的ChatGPT和GPT-4，解码器专用的语言模型近期增长迅猛。

如何优化用于程序修复的LLMs？通常，LLMs从广泛的数据集中获取一般性知识。因此，在将现成的LLMs与自动程序修复（APR）结合时，一个基本的研究问题出现了：如何将通用的LLMs适应于特定的程序修复任务。图5显示了LLM为基础的APR研究中三种常见适应策略的普及情况：微调、少样本学习和零样本学习。我们的研究结果表明，零样本学习在48%的研究中被采用，是最流行的方法，表明了将LLMs原样用于程序修复任务的趋势。同时，微调在37%的情况下被使用，其次是少样本学习，占15%。微调是指LLMs在较小的、任务特定的数据集上进一步训练的过程。这是一种直观的方法，可以让LLMs通过监督学习调整其权重和偏差，使它们能够在与其初始训练相似但并非完全相同的新任务上表现如预期。在APR社区，当具有数百万参数的LLMs（如T5和CodeT5）初现时，微调被广泛应用，因为它可以显著提高目标程序修复任务的性能，而无需从头开始训练LLM。

我们将现有的自动程序修复研究总结为LLM的微调过程，分为三个阶段。首先，研究人员直接将程序修复视为一项特定数据集下游任务，例如微调T5, CodeT5, CodeBERT和GPT-2。第二，研究人员利用更先进的微调策略以获得更好的性能。例如，CIRCLE利用持续学习来使用单一模型修复多种语言，而RAP-Gen利用检索增强生成来引导补丁搜索空间。最近，Zirak等人在APR中以两个LLM，即TFix和CodeBERT，以及三种微调方法，即全微调、使用轻量级适配器层微调和课程学习，进行了实证研究探索领域转移问题。第三，研究人员进行实证研究，探索不同修复场景中各种LLM的实际修复能力。例如，Zhang等人微调五个LLM修复C/C++安全漏洞，Wu等人涉及四个LLM修复Java漏洞，而Jiang等人则考虑四个LLM修复Java语义错误。

少样本学习。少样本学习是指大型语言模型（LLMs）在仅有非常有限的数据量（通常仅有少数示例）情况下学习或适应新任务的能力。这是一种有效的方法，通过示例帮助LLMs理解目标任务，并生成适当的响应，而无需进行显式的重新训练或微调。在应用程序漏洞修复（APR）领域，少样本学习通常被广泛应用于LLMs中，并且在具有数十亿参数的情况下效果显著，因为它要求LLMs具有从非常有限数据中泛化的强大能力。研究人员通常在输入提示中直接向LLMs提供少量的修复示例，并要求LLMs生成正确的修补程序。例如，Nashid等人[108]为CodeX构建有效提示，通过检索类似的修复演示，而Xia等人[162]则向LLMs提供来自相同有错误的项目的示例，以学习编码风格。

5.2.3 零样本学习。零样本学习将少样本学习的概念推进到更深的层次，要求大型语言模型在没有明确示例的情况下进行程序修复。这是一种最近流行的查询大型语言模型执行各种未见过任务的方法，其中大型语言模型被提供一个任务描述，必须利用其现有的知识和理解生成一个响应或解决方案。在自动程序修复领域，零样本学习随着创纪录参数的大型语言模型的出现而迅速崛起，ChatGPT 是一个典型的例子，因为它需要一个强大的基础模型来执行类似人类对话的功能。

在零-shot 学习环境中，通常有两种典型的开发路线利用LLMs进行程序修复。第一种是填空式修复，即将程序修复重新构建为填空式任务，然后利用修复模式（如AlphaRepair、GAMMA、FitRepair和Repilot）来帮助LLMs预测部分正确的代码。第二种是基于对话的修复，即通过构建包含各种有价值信息的复杂提示（例如有bug的代码、故障诊断甚至执行反馈），然后与LLMs对话生成正确的补丁。这些修复路线通常需要LLMs具备处理长文本提示和类人对话的能力，因此主要采用具有数十亿级参数的强大LLMs，如ChatGPT和GPT-4。

此外，零-shot 方法摆脱了训练数据集的限制，因此可以泛化到各种修复场景，在其中1:14 Quanjun Zhang, Chunrong Fang, Yang Xie, Yuxiang Ma, Weisong Sun, Yun Yang, 和 Zhenyu Chen SemanticBug 占48%，SecurityVulnerability 占14%，ProgrammingProblem 占9%，StaticWarning 占7%，SyntaxError 占6%，HardwareBug 占3%，TypeError 占3%，PerformanceBug 占2%，SmartContract 占2%，CrashBug 占1%，WebUItest 占1%，APIMisuse 占1%， TestCase 占1%，TranslationBug 占1%，MotionPlanner 占1%，GitHubIssue 占1%，FormalProof 占1%，CodeReview 占1%。如图6所示，LLM-based APR 论文在不同 Bug 类型中的分布情况。

收集训练数据存在挑战或者是不可能的情况，比如硬件故障、深度学习程序以及崩溃错误。

总的来说，微调（fine-tuning）是一种使用监督学习来使LLMs更贴近于程序修复特定细节的方法，在早期出现的百万参数级别模型（比如CodeT5）带来了流行。少样本学习展示了LLMs从少量示例中泛化的能力，在之后出现的十亿参数级别模型（比如Codex）中变得流行。零样本学习展示了LLMs在没有任何先前直接暴露于训练或示例的情况下处理程序修复任务的能力，尤其是最近出现的具有数十或数百亿参数的模型（如ChatGPT和GPT-4）。

RQ2研究结果摘要：(1)我们总结了已经用于修复软件缺陷的46种不同LLM模型，根据模型架构将这些LLM分为三类：仅编码器、编码器-解码器和仅解码器。(2)仅解码器的LLM模型是最常用的，其中有四种是最受欢迎的LLM模型。(3)在现有的基于LLM的程序修复研究中，ChatGPT、GPT-4、CodeT5和Codex是最受欢迎的LLM，分别被使用了37次、25次、23次和21次。(4)我们总结了三种利用LLM中蕴含的海量知识来进行具体程序修复任务的典型方法，即微调、少样本学习和零样本学习。

研究了利用大语言模型 (LLMs) 进行程序修复的应用场景。我们对涉及LLMs的18种现有修复方案进行了全面分析，遵循了Monperrus等人和Zhang等人的分类方法，包括语义错误、语法错误和静态警告。图6展示了LLMs应用于不同修复方案的研究分布情况。我们发现，由测试套件触发的语义错误吸引了LLM为基础的自动程序修复研究最高的关注度，占总研究量的约48%。这种现象在传统基于学习的自动程序修复研究中也有所体现，主要受到Defects4J等知名基准测试的推动。安全漏洞占研究比例约14%，突显了LLMs在应对网络安全挑战方面的重要性。第三大研究数量出现在编程问题领域，占总研究量的约9%。我们还发现，对之前研究通常忽略的罕见bug类型，如静态警告（7%）、语法错误（6%）和硬件bug（3%），表现出越来越高的兴趣。这突显了由于LLMs从海量数据中获取的通识知识，研究人员已经开始探索之前研究中未曾涉及的修复场景。

6.1 语义错误
语义错误指的是程序在语法上正确，但代码并没有实现程序员的意图，尽管它可能会编译和运行而不会出现语法错误。

考虑到绝大多数基于LLM的主题评阅研究集中在语义错误领域，我们将根据LLM的三种使用方式总结这些代表性研究。

6.1.1 精细调整修复。早在2021年，Drain等人引入了DeepDebug，该工具通过在真实GitHub存储库中挖掘的Java方法上精细调整BART模型来学习修复漏洞。2022年，Yuan等人提出了CIRCLE，这是一种基于LLM的多语言自动程序修复框架，通过对预先训练的T5进行持续学习和精细调整。特别地，CIRCLE利用基于难度的复习策略实现终身学习，而无需访问所有历史数据，并使用弹性正则化技术来解决灾难性遗忘问题。CIRCLE是第一个在持续学习环境中使用单一修复模型修复多种编程语言漏洞的方法。Hao等人通过对CodeT5和GraphCodeBERT进行精细调整，并结合课程学习框架提出了APRFiT。2023年，RAP-Gen进一步改进了通过检索增强生成框架的精细调整LLM的修复性能。RAP-Gen首先利用混合式补丁检索器从外部代码库中搜索相关修复模式，然后利用Codet5作为基础模型，通过增强输入，即检索到的修复模式和原始有缺陷的代码，来生成候选补丁。

在使用少样本学习进行修复。在少样本学习中，大型语言模型接受一个提示作为输入，其中包含自然语言指令、少量任务演示示例和一个查询，然后生成一个输出。2023年，Nashid等人提出了一个名为CEDAR的检索式提示选择方法，用于多个代码生成任务，包括程序修复。CEDAR首先基于嵌入或频率相似性自动检索与有bug代码相关的修复演示，并创建提示询问Codex以按少样本学习的方式生成正确的补丁。与此同时，Do等人通过使用ChatGPT和Galpaca-30b（LLaMA的一个变体）进行了一个少样本示例生成管道的初步研究。

6.1.3 使用零样本学习进行修复。与上述基于微调的自动程序修复工作依赖于高质量的bug修复代码对不同，一些方法提出直接利用现成的LLMs，无需进行任何训练。这些方法尝试在零样本设置下生成补丁，可分为两种类型，即填空式自动程序修复和对话式自动程序修复。

Cloze式自动程序修复（APR）利用LLMs的预训练目标（例如，模型语言建模）来通过修复模式帮助预测正确的代码标记。2022年，夏等人[163]推出了AlphaRepair，这是一种Cloze式自动程序修复工具，可以直接查询LLMs来在零样本学习环境中生成补丁。AlphaRepair首先用一个掩码行替换有bug的行，并查询CodeBERT填充掩码行以生成候选补丁。此外，FitRepair[161]是一种基于CodeT5和塑身手术假设的更先进的Cloze式APR方法。FitRepair利用知识增强微调和修复导向微调，帮助CodeT5分别学习有bug的项目特定知识和Cloze式任务知识。然后，FitRepair通过静态分析检索相关标识符，将其馈入经过微调的CodeT5以生成候选补丁。类似地，Repilot[154]通过一个补全引擎提高了Cloze式APR AlphaRepair。Repilot构建了LLMs和一个补全引擎之间的交互，通过首先修剪LLMs建议的不可行标记，然后根据补全引擎提供的建议完成标记来生成更有效的补丁。GAMMA[186]进一步探索了在零样本学习场景中使用LLMs生成补丁的潜力，其中包含一系列精简总结的修复模板。特别地，GAMMA试图解决传统基于模板的APR（例如TBar [92]）中的捐赠代码检索问题，并将补丁生成视为一项填空任务，通过查询LLMs来预测预定义修复模式中掩码标记的正确代码。与Cloze式APR不同，Ribeiro等人[122]将APR问题框架化为一个代码完成任务，并应用CodeGPT来修复ManySStuBs4J [72]中的错误。

我们用对话式的自动程序修复（APR）技术，利用了大型语言模型（LLMs）强大的自然语言理解和编程语言理解能力，通过测试失败信息来迭代生成修补程序。

在2023年，夏等人提出了ChatRepair，这是第一个采用对话方式进行代码自动修复（APR）的方法，该方法将补丁生成与即时反馈交替进行，以实现自动代码修复。

ChatRepair 构建提示与测试失败信息，并查询ChatGPT生成先前不正确但合理的修补程序。然而，ChatRepair 主要依赖负面反馈（即从失败测试中导出的失败信息）来引导对话，这可能并不总是提供针对有效修复的具体和足够的提示。因此，Kong 等人 [75] 提出了 ContrastRepair，该方法包括来自通过测试的正面反馈，以补充负面反馈。给定一个有 bug 的程序和一个失败的测试用例，ContrastRepair 通过对失败的测试用例进行微小修改生成一个类似的通过测试用例。然后，ContrastRepair 对LLMs构建一个对比对，帮助它们更好地找出 bug 的根本原因并生成准确的修补程序。在上述的对话式APR场景中，LLMs 将一个包含关于有 bug 代码的标记的提示作为输入，并推断关于修补程序的后续标记作为输出。在对话过程中，输入提示和输出答案中的所有标记都消耗了计算和金融成本，例如对于GPT-4，每1000个输入标记0.06美元，每1000个生成标记0.03美元。为了降低ChatRepair的计算成本，Hidvegi 等人 [52] 提出了CigaR，一种以 token 效率为基础的LLM的APR方法，专注于最小化ChatGPT的 token 成本。CigaR设计了三个提示，帮助ChatGPT通过先前的响应最小化整体 token 成本，包括（1）用于初始化修复过程的启动提示，（2）用于优化部分修补程序的改进提示，避免丢弃可能有价值的修补程序，（3）一个乘法提示建立在已经生成的合理修补程序基础上，通过最大化多样性合成更多合理的修补程序。不同于以前依赖固定提示模板的工作，RepairAgent [12] 将ChatGPT视为一个能够自主规划和执行生成修补程序的行动的代理，利用动态提示和状态机来选择合适的工具。

除了上述的新技术外，研究人员已经进行了大量经验研究，探索了LLMs在修复语义错误方面的能力。早在2021年，Mashhadi等人[100]初步评估了将CodeBERT微调用于修复ManySStuBs4J中Java错误的性能。Lajko等人[77]通过实证方法对GPT-2进行微调，自动生成JavaScript错误的候选补丁。Prenner等人[118]研究了在零-shot情况下使用CodeX修复QuixBugs，并且Sobania等人[131]利用更强大的LLM ChatGPT进行提示工程，为QuixBugs生成补丁。2023年，Horvath等人[54]探究了模型架构和程序表示的影响，涉及两种流行的编程语言，即Java和JavaScript，三种不同的代码表示，即原始文本，命令序列和ASTs，以及四种LLMs，即T5，CodeT5，RoBERTa和GPTNeo。与此同时，Zhao等人[194]进行了一项全面的调查，探讨了LLMs在修复代码审查错误方面的有效利用，在顶尖方式（如ChatGPT和GPT-4）及以微调方式（如InCoder，CodeT5+，CodeFuse，LLaMA，CodeGen-2，CodeLLaMA）涉及六种LLMs。最近，受到自然语言中语法错误可以通过往返翻译来修复的现象启发，Ruiz等人[125]研究了RTT流程在零-shot方式下修复代码中的错误的程度。这一全面研究涉及八种LLMs：PLBART，CodeT5，TransCoder，SantaCoder，InCoder，StarCoderBase，GPT-3.5 和GPT-4，以及四种APR基准：Defects4J-v1.2，Defects4J-v2.0，QuixBugs 和HumanEval-Java。

此外，在顶级软件工程会议上会发表更加全面的实证研究。

Xia等人进行了一项全面的研究，探讨LLM在程序修复中的性能，涉及两类LLM（即填充和生成模型）中的九种，以及跨三种编程语言的五个数据集。他们探索了三种使用LLM生成补丁的方式：完整函数生成，正确代码填充和单行生成。同时，Jiang等人在零-shot和微调设置下，经验性地探索了四个LLM的十种变体在四个Java基准测试中的修复能力。他们还构建了一个新基准测试，HumanEval-Java，这些LLM在训练期间没有见过，以解决数据泄露问题。Huang等人在微调范式中进行了一项LLM修复能力的经验性研究，涉及五种LLM、三种编程语言和三种修复场景。

6.2 安全漏洞
软件漏洞是指软件设计、实现、操作或管理中的缺陷、错误或弱点。一旦被利用，漏洞可能导致未经授权的访问、篡改或破坏软件的预期功能。2022年，Fu等人提出了VulRepair，这是一种基于LLM的自动程序修复方法，通过使用漏洞数据集对CodeT5进行微调。此外，在2023年，考虑到一个易受攻击的函数只包含几个需要修复的核心元素，Fu等人提出了VQM，一种基于LLM的自动漏洞修复方法，利用基于VIT的目标检测方法帮助CodeT5在修补生成期间更多地关注易受攻击的代码区域。受语义bug与安全漏洞之间关系的启发，Zhang等人提出了一种基于迁移学习的增强LLM漏洞框架，展示了对VulRepair更优越的性能。在2024年，Zhou等人提出了VulMaster，这是一种基于CodeT5的方法，用于解决VulRepair的几个局限，即(1)通过融合decoder框架来处理较长的易受攻击代码；(2)通过AST来捕获易受攻击代码的结构；以及(3)通过CWE系统来理解专家的漏洞知识。De等人利用量化低秩适应（QLoRA）对两个先进的70亿参数的LLM，即CodeLlama和Mistral进行微调，以修复C语言漏洞。

实证研究。Pearce等人评估了LLMs在以零次伤害方式修复漏洞中的表现，涉及到五款商用黑盒和两款开源LLMs。他们特别研究了设计查询LLMs以修补易受攻击代码的提示的潜力。此外，Zhang等人进行了一项实证研究，调查了对漏洞修复进行微调LLMs的表现，涉及到来自三类的五款LLMs、两个C漏洞数据集和100多个经过训练的LLMs，这是迄今为止最大的数据集。他们还探索了ChatGPT在以零次伤害方式修补安全漏洞的潜力。

同时，吴等人（159）以零-shot 方式和 fine-tuning 方式探索了五种语言模型在两个真实的Java漏洞数据集上的修复能力，以及四种LLMs。最近，乐等人（78）进行了一项关于ChatGPT和Bard在检测和修复JavaScript程序中安全漏洞方面的初步研究。静态警告是指由静态工具生成的自动警报，这些工具检查代码以识别潜在错误、低效性或安全漏洞，而不执行程序。2021年，贝拉比等人（7）介绍了 TF ix，被认为是将 LLMs 融入APR中的先驱。

受T5的启发，TFix将程序修复问题归纳为对代码序列的文本到文本任务，即给定一个作为文本的编码错误，预测一个新的文本作为修补程序。TFix是从T5中特别优化而来的，使用了一个高质量数据集，包括 ESLint 静态分析器检测到的 52 种错误类型的 100,000 个bug修复对。与RAP-Gen和InferFix类似，InferFix使用检索增强生成方式来优化语言模型用于程序修复，即(1)利用检索器从外部代码库中搜索语义等价的错误和相应的修复，以及(2)在受监督的bug修复数据集上对语言模型进行微调，通过添加检索到的相似修复来增强提示。

然而，RAP-Gen 专注于解决语义错误，而InferFix修复静态分析器Infer检测到的静态错误；RAP-Gen利用CodeT5，而InferFix使用了一个拥有120亿参数的更大模型Codex。最近，Wadhwa等人提出了CORE来解决静态分析工具标记的代码质量问题。CORE利用ChatGPT作为提出候选代码修订的LLM，再使用GPT-4作为排名者，在把候选修订呈现给开发人员之前对它们进行排名。Alrashedy等人引入了FDSP来以自我调试的方式解决静态代码分析工具Bandit检测到的修复安全漏洞。

除了上述的技术论文之外，Kim等人[73]实证地调查了TFix在工业三星Kotlin项目中修复由静态分析工具SonarQube检测到的错误的表现。Mohajer等人[104]在静态代码分析领域进行了一项更全面的研究，并提出了SkipAnalyzer，一种基于LLM的强大工具，用于执行三项相关任务：检测错误、筛选假阳性警告和修补检测到的错误。

6.4 语法错误指的是当代码不遵循编程语言的规则或语法时发生的解析错误，例如无效的语句和表达式。这些错误在程序执行之前由编译器或解释器在解析阶段检测到。Ahmed等人提出了SYNSHINE，通过使用编译器诊断对RoBERTa进行预训练和微调来修复语法错误。RustAssistant利用GPT-3.5和GPT-4通过提示工程来修复Rust编译错误。RING是一种多语言语法错误修复方法，由Codex和少样本学习驱动，在六种不同语言中表现出很大的潜力。PCR-Chain尝试根据ChatGPT和Chain-of-Thought Prompting解决完全合格的名称并修复最后一英里的语法错误。

类型错误发生在将一个操作或函数应用于不适当类型的对象时。

这意味着操作期望接收特定的数据类型，但却收到了另一种数据类型，从而导致异常。在动态类型语言（例如Python和JavaScript）中，这些错误比较常见，因为这些语言不允许使用不适用于给定数据类型的操作。

在2024年，Chow等人构建了PyTyDefects数据集，包含了来自176个GitHub仓库的2,766个类型错误修复对。他们还引入了PyTy，这是一种基于T5的修复技术，通过使用PyTyDefects对Python中的类型错误进行微调，修复了TFix的通用LLM。与基于微调的方法PyTy不同，TypeFix是一种面向Python类型错误的基于提示的修复方法，以零-shot方式工作。类似于GAMMA，TypeFix通过填充代码提示中的修复模板中的掩码，查询LLMs以生成补丁。然而，TypeFix通过层次聚类算法自动挖掘这些模板，而不是像GAMMA那样依赖预定义的模板。此外，Ribeiro等人提出了Mentat，一种基于GPT-3的OCaml程序类型错误修复技术。Mentat首先分析源代码生成上下文相关提示，然后利用GPT-3的高级语言理解和生成能力生成潜在的修补程序。

2022年，张等人[179]提出了MMAPR，通过Codex和few-shot learning来修复初学Python编程时出现的语法和语义错误。范等人[34]系统评估修复技术是否能够修正LeetCode比赛中LLMs生成的不正确解决方案。类似地，刘等人[93]分析ChatGPT生成的Java和Python代码在来自LeetCode的2,033个编程任务中的正确性，并通过不同提示查询ChatGPT来减轻不正确的代码。最近，张等人[187]经验性地评估了ChatGPT修正编程问题不正确提交的表现。类似的研究由Haque等人[51]和Tian等人[138]进行。石庄等人[63]将ChatGPT和GPT-4两个LLMs与Refactory[58]结合起来，用于修复编程作业中的问题。他们利用LLMs来修复程序中的语法错误和Refactory无法修复的程序。最近，赵等人[192]探索LLMs，包括ChatGPT、StarCoder和CodeLlama，如何在修复高级编程课程学生作业方面的表现。

他们构建了一个名为Defects4DS的学生作业数据集，其中包含了来自4个编程作业的682个提交，并引入了一个名为PaR的修复框架，通过选择同学们的解决方案来生成修补程序来修复问题。

性能bug指的是不影响功能但会导致代码运行效率低下，从而消耗不必要的时间和资源的代码片段。Garg等人提出了DeepDev-PERF，这是第一个基于LLM的方法，用于通过对BART模型进行微调来修复C#应用程序中的性能bug。在DeepDev-PERF的基础上，RAPGen进一步通过迅速工程优化了修复过程，相比传统的繁琐微调方法表现出更高的效率。RAPGen从知识库中检索相关指令，构建一个针对特定bug的输入提示，然后使用Codex生成优化后的代码。

这种方法类似于 CEDAR，但通过独特的提示设计实现更有效和资源高效的解决方案。

硬件漏洞修复的研究在软件领域已经得到很好的探索，但对硬件描述语言（如Verilog）的研究却相对较少。Ahmad等人[1]通过实证研究探索了LLMs在硬件设计中修复安全相关漏洞的潜力。他们利用即时工程技术查询Codex、ChatGPT、GPT-4和CodeGen，针对10种常见漏洞CWE提出了15个安全相关漏洞。受检索增强生成启发，Tsai等人[141]引入了RTLFixer，这是一个利用LLMs和迭代提示来修复Verilog语法错误的框架。RTLFixer制定了一个输入提示以查询GPT-4来生成正确的代码。接着，RTLFixer利用编译器的错误日志和检索数据库中的人类指导作为反馈，进行交互式调试循环，直到所有错误被解决。华为的Yao等人[173]专注于芯片设计领域，提出了HDLdebugger，这是一个由LLMs辅助的硬件描述语言调试框架。该框架包括通过逆向工程实现数据生成、一个搜索引擎以增强检索增强生成，以及一个微调方法来训练检索增强LLMs。

同样地，付等人[40]关注硬件设计迭代过程，并介绍了LLM4SECHW，这是一个基于LLM的硬件调试框架。LLM4SECHW从开源硬件项目构建了一个面向硬件调试的数据集，并微调了一套硬件领域特定的LLM模型，能够自动阅读硬件设计并修复错误。

智能合约是区块链网络上的自执行合约，协议条款直接写入代码中。2023年，Napoli等人[107]进行了一项初步研究，探讨ChatGPT在修复143个易受攻击的Solidity代码的能力。

2024年，张等人提出了ACFIX，这是一种基于GPT的方法，用于修复基于静态代码切片和Chain-of-Thought Prompting的智能合约中的访问控制漏洞。ACFIX挖掘了主要代码功能类别的常见AC实践。这些实践构成了一个知识库，指导LLMs修复具有类似功能的代码。

6.10 杂项维修类型 我们汇总了不足两篇论文涵盖的其他维修情景如下。

崩溃漏洞。崩溃漏洞被认为是重要问题，因为它们可能导致程序出现意外行为并终止。Du等人进行了首次调查，研究ChatGPT解决真实世界崩溃漏洞的能力，特别是与代码和环境相关的漏洞。他们设计了不同的提示并提出了IntDiagSolver，一种涉及与LLM持续交互的交互方法。

API误用。张等人进行了一项实证研究，探讨了基于学习的自动程序修复（APR）技术在API误用修复方面的性能表现，涉及九种具有数百万参数的LLMs，在微调设置下进行研究，即CodeBERT、GraphCodeBERT、CodeGPT、PolyCoder-160M、PolyCoder-0.4B、CodeTrans、PLBART、CodeT5和UniXcoder。

Xu等人进行了第一项关于结合传统Web用户界面测试修复技术和ChatGPT以增强Web用户界面测试修复的可行性研究。他们首先利用传统的修复方法生成候选匹配元素的初步列表，并使用ChatGPT执行全局匹配，进一步选择最佳匹配元素。

翻译瑕疵。代码翻译是指将源代码从一种语言转换为另一种语言的过程，而不改变原始程序的功能或逻辑。潘等人进行了一项大规模的实证研究，调查了大型语言模型在代码翻译方面的能力，涉及 C、C++、Go、Java 和 Python 等五种不同语言。他们提出了关于翻译失败的漏洞分类，共包括 15 个类别和五组，并设计了一种迭代式的翻译漏洞修复方法，用一组启发式方法修复大型语言模型的翻译失败，并提供了及时制定的启发式方法。

Yaraghi等人在其论文中介绍了一种名为TARGET的基于LLM的方法，用于修复破损的测试用例，将测试修复视为一项语言翻译任务。他们首先构建了TARBENCH，一个包含45,373个在59个开源项目中的破损测试修复的综合基准测试集。

然后，他们对现成的LLM（即PLBART，CodeGen和CodeT5+）进行微调，以生成带有关键代码背景的正确测试代码。

在自动驾驶汽车中，运动规划算法指的是为汽车计算一条可行路径的算法，使其能够在预定时间范围内从初始状态行驶到指定的目标区域。

Lin等人[90]提出了DrPlanner，这是第一个利用GPT-4诊断和修复运动规划器的自动化框架。DrPlanner首先设计了一个结构化提示，用自然语言和编程语言来描述规划器。然后，DrPlanner以闭环方式查询GPT-4，通过持续的诊断反馈生成算法来修复问题。一部关于大型语言模型用于自动化程序修复的系统性文献综述。图7展示了各个数据集中的出版物分布。

图 8. 不同输入形式在出版物中的分布比例：
- 提示输入 52%
- 原始输入 18%
- 对话式输入 18%
- 掩饰式输入 9%
- 结构感知输入 3%

软件形式证明。形式软件验证旨在验证软件属性的正确性。First等人[36]引入了Baldur，这是一种基于LLM的方法，用于自动化生成和修复整个形式证明。Baldur利用了两个版本的Minerva[81]，它们是基于PaLM[20]的数学语料库进行预训练的：一个是具有80亿参数，另一个是具有620亿参数。Baldur首先微调生成模型，以合成证明数据集上定理的整个证明，然后根据证明助手的错误消息微调修复模型，以修复生成错误的证明。

在 Github 上的问题。不同于大多数以前的研究（第16篇），那些研究评估了大型语言模型在解决独立问题（如编程问题）时的表现，Jimenez等人（第68篇）探索了在真实软件工程环境中利用大型语言模型解决 Github 问题的潜力。给定一个提交到流行的 Github Python 仓库的问题（比如一个错误报告或一个功能请求），他们微调 CodeLlama-7B 和 CodeLlama-13B，生成一个能通过单元测试和系统测试的补丁。

Guo等人进行了第一项经验研究，探讨了ChatGPT在代码审查中的潜力，重点关注基于现有代码审查的自动代码改进。他们利用提示工程来比较ChatGPT与CodeReviewer使用两个数据集：一个名为CodeReview的已建立数据集和一个名为CodeReview-New的新引入数据集。他们还设计了几种策略来提高ChatGPT的性能，比如使用更先进的模型。

研究问题3的综合结果总结显示，在文献中发现LLMs被应用于各种维修场景中，涉及18种错误类型。在一些常见的由传统APR主导的情景中，比如语义错误，研究人员继续致力于探究LLMs的应用。此外，由于LLMs从各种可能的互联网数据中学习的通用知识，基于LLMs的APR已经扩展到一些以前未被探索过的罕见场景，比如硬件错误和Web UI。

研究问题4: 哪些关键因素有助于LLMs用于APR的整合? 使用哪些数据集来评估LLM APR研究？基准测试对于评估APR技术的性能至关重要，从根本上塑造了研究社区内发展方向。例如，在过去十年中，Defects4J已成为一个普遍做法，为了解所提出方法的优势和劣势，指导研究人员解决该领域中最紧迫的挑战，促进了一个标准的比较场景。我们确定了来自所有收集研究中的78个基准测试，并展示了在图7中多次使用的基准测试。

我们发现，来自以前自动程序修复研究的现有基准测试集Defects4J（引用了28篇论文）是最受欢迎的，其次是QuixBugs（引用了20篇论文）和BFP（引用了12篇论文）。这种现象是合理的，因为现有的基准测试集通常构建得很好，已经被社区检验和使用，因此成为评估新的自动程序修复技术的首选。例如，大量基于LLM的自动程序修复技术是通过现有的流行基准测试集进行评估的，比如CIRCLE、AlphaRepair和GAMMA。

此外，我们注意到LLM（Large Language Model）驱动的自动程序修复中出现了一些新构建的基准，例如TFix（六篇论文），HumanEval-Java（五篇论文）和Pearce等人（两篇论文）。我们将这些新基准总结为三类。第一类数据集旨在针对罕见的修复场景。正如在第6节中讨论的那样，LLMs已经被用于各种场景，其中一些以前的工作尚未考虑，导致了相关基准的空白。因此，随着基于LLM的自动程序修复技术的出现，研究人员还开发了相应的新数据集，例如TFix（用于静态警告），DeepDev-PERF（用于性能错误），Du等人（用于崩溃错误），以及Zhang等人（用于API错误使用）。第二类数据集试图解决以前基准的局限性。例如，考虑到缺乏测试套件的BFP，FixEval提供了一个大规模的竞赛编程问题的单元测试集合，并使用PLBART和CodeT5进行评估。第三类数据集旨在解决LLMs独特的问题，特别是数据泄漏问题。例如，Jiang等人创建了一个新的评估基准HumanEval-Java，这是LLMs在预训练期间没有见过的。Zhang等人在APR领域广泛探讨了ChatGPT的数据泄漏问题，并引入了EvalGPTFix，这是ChatGPT在训练截止点后从竞争编程问题中产生的新基准。DebugBench是EvalGPTFix的后续版本，涵盖规模更大、类型更多样的错误。DebugBench包含来自LeetCode社区的4,253个错误程序，覆盖C++、Java和Python中的四个主要错误类别和18个次要类型。类似地，ConDefects包含了来自在线竞赛平台AtCoder的1,254个Java有错误程序和1,625个Python有错误程序。这些收集到的程序产生于2021年10月至2023年9月，旨在解决基于LLM的自动程序修复方法的数据泄漏问题。与前述源自编程问题的基准不同，Silva等人提出了GitBug-Java，这是一个可重现的基准，包括了199个最近的Java错误。这些错误是从55个知名开源代码库的2023年提交历史中提取的，以减轻数据泄漏的风险。

当使用LLMs时，软件错误会被转化为什么输入形式？LLMs（大型语言模型）具有强大的自然语言理解能力，使得基于LLMs的自动程序修复（APR）的输入包含丰富信息，因此比传统的APR技术更复杂。

我们根据数据类型将各种输入形式总结为五类。如图8所示，在收集的论文中，我们发现52%利用提示工程将bug修复信息提供给LLMs，18%利用对话风格的表述提供动态信息。在一项关于大型语言模型用于自动程序修复的系统文献综述中，我们还发现只有18%的基于LLM的APR研究采用类似传统DL/ML模型的原始bug修复输入方式[182]。接下来，我们将讨论LLMs使用的五种输入表达形式。

修复缺陷的原始输入。与大多数传统的基于学习的程序错误修复相似，这种类型的输入将程序错误修复视为一个NMT任务，它将一个句子从一个源语言（即有错误的代码）翻译成另一种目标语言（即正确的代码）。这种表示直接将有错误的代码片段提供给LLMs，并通常用于训练LLMs进行语义错误[28,100,202]、安全漏洞[39,185]和静态警告[73]的监督式学习。例如，张等人[185]研究了三种程序错误修复表示（即上下文、抽象和记号化）的性能，用于微调五个LLMs进行漏洞修复。

"带提示输入"。这种输入类型在问题代码中融合了更多信息。提示将不同的输入部分与一些前缀的提示连接起来，有效地弥合了预训练任务与下游任务之间的差距。例如，CIRCLE【176】利用手动设计的提示模板将错误代码和相应上下文转换成统一的填空格式。特别地，他们使用“错误行:”和“上下文:”来表示错误和上下文代码，并使用“The fixed code is:”来查询一个基于T5的模型，根据之前的输入生成候选补丁。此外，TFix【7】，Zirak 等人【202】和 Kim 等人【73】将关于 bug 的所有有价值信息表示为一个单独的文本，包括 bug 类型、bug 消息、bug 行和 bug 上下文。此外，InferFix【69】和 RAP-Gen【151】通过从外部代码库中检索相关的修复示例来构建提示。

❸掩码输入。这种输入方式会遮盖有漏洞的代码，并查询LLM以填充正确的代码令牌。与上述输入形式不同，掩码输入将APR问题重新构造为一项填空式任务，并直接利用LLM的预训练目标来进行零-shot设置。 AlphaRepair【163】被认为是展示掩码输入潜力的第一项工作，研究人员已经提出了各种后续工作来更好地执行用于修补生成的掩码预测，例如具有良好总结修复模式的GAMMA【186】，具有整形手术假设的FitRepair【161】，具有一个完成引擎的Repilot【154】，以及实证研究【65,162】。

这种输入方式进一步通过反馈驱动的对话，如人类一样，扩展了提示输入。对话式表示包含了更复杂的信息，比如动态执行结果，同时通过多轮对话不断改进生成的修补程序。例如，Sobania等人在一项研究中，早期探索了利用ChatGPT对话功能进行程序修复的可行性，激发了一些后续研究。

结构感知输入。这种输入将源代码表示为语法结构，例如抽象语法树（ASTs）。举例来说，Horvath等人利用RoBERTa和GPTNeo对程序修复的AST进行编码。此外，VulMaster利用AST作为其输入的一部分，捕捉易受攻击代码的结构特征。

在支持补丁正确性方面，LLMs如何被使用？最近APR研究的关注点是由于社区中主流的测试驱动生成和验证修复工作流。特别是，在生成候选补丁后，开发者编写的测试套件被用作验证补丁正确性的标准，通过的补丁则被返回给开发者。然而，作为不完全的规范，测试套件只描述程序行为空间的一部分，导致出现可能的过度拟合补丁而未修复错误。因此，开发者需要花费大量精力手动筛选过度拟合的补丁，甚至影响调试性能。研究人员提出了各种自动化补丁正确性评估（APCA）技术，以确定一个可能的补丁是否过度拟合，以提高返回补丁的质量。

举个例子，PATCH-SIM通过计算动态执行跟踪的相似性来评估补丁的正确性。PATCH-SIM被认为是在 APCA 领域中的一项基础工作，为后续的工作提供了关键指导，尤其是基于学习或LLM的工作。

表5. 利用LLMs预测补丁正确性的现有APR研究总结。

2020年，田等人的LLMs代码库中使用了BERT模型，项目链接为https://github.com/TruX-DTF/DL4PatchCorrectness

2022年，Quatrain项目也使用了BERT模型，项目链接为https://github.com/Trustworthy-Software/Quatrain

2023年，田等人再次使用了BERT模型，项目链接为https://github.com/HaoyeTianCoder/Panther

同样在2023年，Invalidator项目使用了CodeBERT模型，项目链接为https://github.com/thanhlecongg/Invalidator

2023年的PatchZero项目使用了StarCoder模型，具体信息未知。

2024年APPT出现了三种新的模型：BERT、CodeBERT和GraphCodeBERT。可以在https://github.com/iSEngLab/APPT查看更多。表5展示了涉及LLMs的现有APCA研究。我们将它们总结为三个阶段。

2020年，田等人通过实证研究探索了使用表示学习模型作为代码嵌入的性能，用于推断程序补丁的正确性。他们将大型语言模型（LLMs）作为特征提取器。

在参考了PATCH-SIM的相似性基线之后，研究者们首先基于代码嵌入计算了经过修补的和有缺陷的代码片段之间的相似度，然后利用一个二元分类器来预测修补程序的正确性。他们考虑了四种嵌入模型，包括重新训练的模型（例如Doc2vec、code2vec和CC2vec）和预训练模型（例如BERT），这是第一个利用LLMs的APCA研究。最近，Le等人提出了Invalidator，通过语义和句法推理来评估修补程序的正确性。与Tian等人类似，他们利用CodeBERT来提取代码特征并训练一个分类器进行预测。不同于以上研究计算补丁相似性的方法，Tian等人将APCA构建为一个问答（QA）问题，并提出了Quatrain。Quatrain首先利用CodeBERT来编码bug报告和修补描述，并训练一个QA模型进行预测。

2024年，张等人[183]提出了APPT，它采用BERT作为编码器堆栈，接着是一个LSTM堆栈和一个深度学习分类器。与之前的研究不同，之前的研究[135,136]限制了BERT仅用于特征提取而没有训练的好处，APPT进一步对LLMs进行微调，结合其他组件作为一个整体的流水线，从而特别适应于对补丁正确性进行推理。APPT默认使用BERT实现，并且经过验证适用于其他高级LLM，比如CodeBERT和GraphCodeBERT。

在2023年，周等人提出了PatchZero来探索在零-shot设置中使用LLM（大语言模型）预测程序补丁正确性的可行性。PatchZero直接向LLM查询，根据先前的标记生成关于补丁正确性的下一个标记（即“正确”或“过拟合”），这类似于LLM的原始预训练目标。

7.4 LLMs如何在代码生成和程序修复中发挥作用？与通常使用启发式方法或神经网络一次性生成大量补丁的传统APR方法相比，LLMs可以根据动态执行的结果迭代地优化生成的补丁。正如第5.2节中提到的，LLMs带来的迭代式补丁生成能力促进了基于对话的修复技术的出现。除了基于对话的修复，我们还发现一些通过修复LLM生成的代码并获得反馈以改进代码生成性能的努力，称为自我修复。不同于基于对话的修复，自我修复方法利用LLMs来通过研究执行结果并解释生成的代码来识别其自身实施的代码错误。

这种自修复方法结合了代码生成和程序修复，可以看作是迈向自动化编程的进一步步骤。

表6. 使用APR提升代码生成的现有研究摘要。

2023年研究表明，AgentCoder [60]推出了LLMs代码库。这个新的代码库包含了GPT-3.5、GPT-4、Claude以及PaLM等最新的模型。

2023 年的自我编辑项目包括 InCoder、CodeGen 和 Codex，代码托管在 GitHub 上：https://github.com/zkcpku/Self-Edit。2024 年的自我调试项目涉及 ChatGPT、GPT-4、Codex 和 StarCoder，详情待定。

2024 年郑等人[195] 发布了 CodeLLaMA 和 DeepseekCoder。您可以在 https://github.com/OpenCodeInterpreter/OpenCodeInterpreter 找到该项目。同年 CYCLE [26] 项目也发布了CodeGen 和 StarCoder，您可以在 https://github.com/ARiSE-Lab/CYCLE_OOPSLA_24 找到该项目。另外，2024 年 LDB [196] 项目涉及到 StarCoder、CodeLLaMA 和 GPT-3.5，您可以在https://github.com/FloridSleeves/LLMDebugger 找到该项目。同年 Olausson 等人[110] 发布了 GPT-3.5、GPT-4 和 CodeLLaMA 项目，您可以在 https://github.com/theoxo/self-repair 找到该项目。2024 年胡等人[56] 发布了 GPT-4 项目，具体信息未公开。

表6展示了一些基于LLM的研究，利用程序修复来提升代码生成。

Self-Edit是第一个尝试采用神经编码编辑器的项目，它将生成的代码和错误信息作为输入，以提高竞争性编程任务中的代码质量。Self-Edit使用经过微调的模型（如PyCodeGPT、GPT-Neo、CodeGen、GPT-Neo、InCoder、GPT-J）和基于提示的LLMs（如InCoder、CodeGen、Codex）进行评估。DeepMind的Chen等人提出了Self-Debugging，通过少量提示来教导LLMs调试其自己预测的代码，其中包括Codex、ChatGPT、GPT-4和StarCoder。还有一些类似的自我修复研究，包括OpenCodeInterpreter、Cycle、LDB、SelfEvolve、Self-Refine、胡等人、AgentCoder。最近，Olausson等人进行了一项实证研究，调查了CodeLlama、GPT-3.5和GPT-4在自我修复代码生成方面的能力。他们发现，自我修复并不是代码生成挑战的万灵药，因为现有的LLMs经常无法提供可靠、准确和有价值的有关代码错误的反馈。

在收集的基于LLM的APR论文中，这些论文提供公开可用的研究成果的频率是多少？开放科学通过透明、可复制和可应用的原则在推动科学进步中发挥关键作用。考虑到这些好处，软件工程领域一直在积极推广开放科学原则，并鼓励所有研究人员分享他们的研究成果，从而增强研究结果的可靠性。在这一部分中，我们调查分析的论文在多大程度上使他们的研究成果公开获取。

我们发现80项研究在其论文中提供了复制包，占所有收集研究的62.99%（80/127）。在78项提出新颖以LLM为基础的APR方法的研究中，这是表4中最大的贡献类型，我们发现其中有53.85%（42/78）的研究未能将其工件公开。这使得其他研究者难以验证实验结果、与现有研究进行量化比较，并在不重新发明轮子的情况下开展后续研究。考虑到一些论文尚未发表，我们将关注顶级软件工程领域的会议，即ICSE、ASE、FSE、ISSTA、TSE和TOSEM，并发现86.84%的论文（33/38）提供相关工件并公开，表明高质量论文致力于可重复性。此外，一些研究只提供数据集或训练模型，而没有源代码或必要的说明。总的来说，开放科学在推进基于LLM的APR研究发展中是一个重要挑战，因为许多因素，如数据集、数据预处理方法、源代码、超参数和文件，都可能影响研究的可重复性。因此，我们希望在LLM-based APR社区的研究人员可以提供高质量的开放源工件以便于复制研究。

RQ4 结果总结：我们总结了78个不同的数据集，这些数据集被用来对比测试大语言模型在修复错误方面的表现。

在基于LLM的程序修复中，Defects4J、QuixBugs、BFP、CVEfixes和Big-Vul是最常被采用的。我们把所有收集的论文中的输入形式分为五类：原始bug修复输入、提示输入、掩模输入、对话式输入和结构感知输入。在应用LLM进行程序修复时，提示输入是最常用的形式，这表明设计有效的提示对于利用LLM的自然语言处理能力尤为重要。我们总结了一些利用LLM预测补丁正确性的研究。所有收集的论文中，62.99%的论文已经公开了他们的工件，而顶级软件工程出版物的比例增加到86.84%。

LLM技术在自动评阅系统中的8个挑战与机遇。我们的研究揭示了未来基于LLM技术的自动评阅系统工作面临的关键挑战和实用指导原则。

在完全自主编程中的一部分 - 自动程序恢复（APR）。如第2节所述，现有的APR方法基于“近似正确假设”运行，即有经验的程序员能够编写几乎正确的程序，只需要进行最少的修改来纠正错误并确保符合所有测试用例。这一假设长期以来一直是APR研究开发的基础。然而，LLMs的发展及其在编程中的应用暗示着一个未来，其中APR可以超越其传统边界，朝着与完全自主编程相结合的更全面的方法发展。在这种情况下，APR不仅可以被重新构想为仅仅是一个用于纠正小型编码错误的工具，还可以被看作是自我校正、自我改进系统的一个组成部分，通过迭代地提高自动生成的代码的质量。幸运的是，我们已经观察到了将修复和编程结合在一起的初步探索，然而这些努力还远远不够。例如，Fan等人[34]利用LLMs修复了他们自己生成的错误解决方案，并且最近的研究[165,187]通过动态执行迭代地完善自动生成的代码。

未来，将自动性编程与全自动编程相结合所带来的机遇是巨大的。首先，可以灵活地实现协作的人工智能编程工具，开发人员编写初始代码，然后由大型语言模型（LLMs）不断优化和修复。

对于复杂问题求解来说，大型语言模型(LLMs)提出了一些创新解决方案，这些解决方案可能是人类程序员没有考虑过的。这种合作可以加快开发周期，减轻人类开发者的调试负担，并导致更有创造力和有效性的解决方案。

其次，LLM具有的通用知识赋予它们支持多个下游任务的能力，从而可以弥合代码生成、测试、调试和修复之间的差距。例如，故障定位是修补程序生成的前提，而修补验证可以反映出故障定位的准确性，使这些任务相互关联。因此，有望在统一框架内利用LLM在这些相互关联的任务中的能力，并通过实时反馈来探索其潜力。

越来越多关于LLM修复成本的关注。如第5.1节所述，文献倾向于利用LLM不断增加的规模来实现更好的性能，比如从CIRCLE中的T5-250M到InferFix中的Codex-12B。 孙悟空等 [ 162] 指出较大的模型通常能够为软件漏洞生成更多正确的补丁，这个趋势是合理的。然而，自动程序修复作为与人类高度相关的任务，拥有数以百万、十亿甚至万亿的参数会在开发流程中带来重大挑战。首先，训练拥有数十亿（甚至更多）参数的LLM是一个非常耗时且资源密集的过程。进行这样的训练通常需要昂贵的GPU资源，这使得许多学术和工业研究人员难以获得。例如，在第5.2节中，我们观察到大多数基于LLM微调的自动程序修复研究都使用了CodeT5/T5或类似规模的模型，除了微软的InferFix使用Codex进行微调。然而，InferFix是由全球领先的技术公司微软提出的，并且是使用行业级硬件，即64个32-GB-V100-GPUs进行训练。其次，虽然使用较大模型生成正确补丁的可能性增加了，但补丁生成时间成本也在增加。例如，姜等 [ 65] 演示PLBART平均需要0.70-0.89秒生成一个正确的补丁，虽然CodeGen用更多参数修复的漏洞比PLBART多，但生成一个正确的补丁平均需要3.64-13.88秒。第三，补丁生成时间的增加进一步压缩了可用于补丁验证的时间，开发人员需要花费更多时间等待模型完成推理。例如，石等 [ 129] 演示LLM需要大量的存储空间和运行时内存消耗，加上高的推理延迟，使得这些模型很难集成到现代集成开发环境中，尤其是在资源受限或实时终端设备上。

在未来，为了解决第一个挑战，有望探索 APR 参数高效调整方法的潜力，比如前缀调整和低秩适应。为了解决第二个挑战，研究人员可以优化语言模型的大小，而不会明显影响其性能，例如模型剪枝、量化和知识蒸馏。

为了解决第三个挑战，我们建议采用一些高级策略，比如变异测试[166]或者利用LLMs在验证之前对候选补丁进行排序。

LLM在人类研究中的应用。正如第6部分所总结的，随着LLM的引入，自动程序修复（APR）在修复流行基准测试中的bug数量方面取得了突破性进展，例如Defects4J-1.2上CIRCLE修复的64个bug，到AlphaRepair修复的74个bug，再到GAMMA修复的82个bug。这一进展促使我们考虑LLM是否确实促进了实际调试工作的改进，以及基于LLM的APR对开发人员日常活动具有更广泛的影响。之前的研究[157,188]突出了开发APR工具时缺乏开发人员充分反馈可能会损害其在实际部署中的有效性的潜在风险。然而，我们对软件工程师如何在实际环境中解决软件问题的理解还存在重大差距，包括他们对专用调试工具的使用以及调试技术方面的专业知识。

因此，在未来，研究人员应进行人类研究，以更深入地了解基于LLM的APR工具在人为因素方面的成熟度和可靠性。 可能的方向是调查LLMs是否可以帮助开发人员减少调试成本，如修复更多的错误，加快故障修复过程以及处理更复杂的错误。此外，调查开发人员对LLMs的看法及与其互动，基于他们的实际经验和已建立的调试实践，会是很有价值的。

探索更多和更稀有的修复场景。根据第6节的总结，我们发现大多数现有基于LLM的自动程序修复研究集中在有限数量的错误类型上，尤其是语义错误。然而，还存在一些稀有的修复场景，LLM对其效益较小，例如硬件错误（一篇论文）和并发错误（零篇论文）。关键挑战在于缺乏能够供LLM学习的充足训练数据。

我们建议未来的工作集中在三个可能的方向上，以扩大LLM在更多修复场景中的应用范围，比如软件需求[167]和模糊测试[191]。

首先，迁移学习是一种有效的训练方法，适用于稀有场景。我们可以首先利用丰富的数据在源场景中对LLMs进行微调，然后再利用少量数据将所学知识转移到目标场景。源场景和目标场景应具有相似的数据分布。例如，张全俊等人[185]证明，从修复程序错误中进行迁移学习可以使五个LLMs的漏洞修复性能平均提高9.40%。

其次，利用填空式自动程序修复（APR）与修复模式生成补丁以解决罕见情况是有潜力的。与需要大量标记数据的微调不同，这种方法直接利用专家领域知识（如预定义模式）来引导LLM的通用预训练知识。此外，对于LLM以前未遇到的场景，研究人员可以利用目标场景中的无标记数据，在无监督学习环境中学习项目或测试语言的数据分布。第三，基于上下文学习和提示工程的解决方案是直接查询十亿参数LLM生成目标场景中正确代码的可行方法，因为这些模型非常庞大，并涵盖几乎所有互联网上可用的数据。

与现成的APR集成。如第6节所述，研究人员通常将LLMs作为核心基础，用于设计新颖的维修方法，例如序列到序列学习。同时，社区也发现一些将LLMs视为集成到现有修复工作流程中的组件的探索。这些研究试图增强现成的APR方法的能力，而不是提出新的技术。

举例来说，CURE结合了GPT和CoCoNut来捕捉在APR任务中的代码语法。DEAR建立在DLFix的基础之上，试图通过微调BERT来学习语句之间的修复关系，即识别两个语句是否需要一起修复。最近，GAMMA将LLMs整合到传统基于模板的APR TBar中，通过查询LLMs来生成掩码代码标记，而不是从本地文件检索捐赠代码。这些工作展示了将LLMs与现成的APR技术相结合的潜力，然而目前在这个领域还缺乏更深入的研究。

未来，研究人员可以尝试将LLMs与更传统的自动摘要生成技术结合起来。

举个例子，利用大型语言模型(LLMs)来帮助SMT求解器为基于约束的自动程序修复生成补丁，或者通过LLM生成的潜在补丁来构建启发式算法的搜索空间以进行程序修复，这样做是很有前景的。此外，基于领域的修复技术可以从LLM强大的代码理解能力中受益，因此可以应用于更广泛的修复场景。

例如，我们可以为特定情景设计固定的模板，比如静态警告，然后利用语言模型中包含的通用知识来生成正确的修补程序。

数据泄漏问题。正如在第7.1节中所强调的那样，张等人[187]发现现有的修复基准已经无意中被包含在流行的LLM（例如ChatGPT）的预训练数据中，这是通过网络抓取等方法实现的。例如，ChatGPT能够列举出Defects4J [71]中所有的项目，这是最受欢迎的一个自动程序修复基准之一。研究人员[186]可以通过检查预训练数据与基准数据对比来确认开源LLM的暴露程度。

由于缺乏训练细节，使用更强大的黑匣子LLMs更具挑战性。然而，作为初步探索，它们主要受到涉及错误数量和错误类型种类的限制。例如，它们全部由编程问题中产生，只有规模较小或中等规模的错误解决方案，而没有深入研究涵盖复杂API调用的大规模真实项目，比如Defecst4J [71]。更重要的是，在其他修复场景中存在的数据泄漏问题，比如安全漏洞和API误用，持续被忽视。不同修复场景之间的数据集可能存在重叠，比如Defects4J，它也作为API误用数据集的一部分来源[189]。总的来说，数据泄漏风险给现有工作的基准评估引入偏见，需要研究人员紧急努力来减轻这种情况。

我们建议未来的研究可以朝以下两个方向进行。首先，构建一个大规模基准测试集，其中不含数据泄漏，包含真实世界的项目，以便评估大型语言模型在更实际的调试场景中的实际修复能力。

商业闭源软件、实时更新的编程网站或手动编写的程序可能是潜在的数据来源。其次，考虑到语言模型在应用时所涉及的各种类型的错误，研究人员在进行相关研究时需要考虑并尝试解决数据泄漏风险。

结论：自动程序修复（APR）解决了自动修复软件缺陷这一长期挑战，从而促进了软件测试、验证和调试实践。最近，大型语言模型（LLMs）给APR领域带来了重大变革，已经取得令人瞩目的进展，进一步展示了未来研究的良好前景。

在这篇论文中，我们系统性地回顾了基于LLM的APR技术的现有文献，从LLM、APR及它们的整合视角进行了总结。我们总结了流行的LLM、典型的利用策略和修复场景。我们还讨论了LLM-based APR 社区中的一些关键因素，比如输入形式和自我调试度量。最后，我们概述了几个挑战，如数据泄露问题，并提出了未来研究的潜在方向。